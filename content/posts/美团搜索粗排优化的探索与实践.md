---
title: "美团搜索粗排优化的探索与实践"
date: 2022-09-06T04:33:17+0000
tags: [美团技术团队, 美团平台, 算法, 搜索推荐, 粗排, 机器学习算法]
---

## 1. 前言


众所周知，在搜索、推荐、广告等大规模工业界应用领域，为了平衡性能和效果，排序系统普遍采用级联架构\[1,2\]，如下图 1 所示。以美团搜索排序系统为例，整个排序分为粗排、精排、重排和混排层；粗排位于召回和精排之间，需要从千级别候选 item 集合中筛选出百级别 item 集合送给精排层。



![图1 排序漏斗](https://p0.meituan.net/travelcube/98369798e211806810de1109354cce68121792.png)



从美团搜索排序全链路视角审视粗排模块，目前粗排层优化存在如下几个挑战点：



* **样本选择偏差**：级联排序系统下，粗排离最后的结果展示环节较远，导致粗排模型离线训练样本空间与待预测的样本空间存在较大的差异，存在严重的样本选择偏差。
* **粗排精排联动**：粗排处于召回和精排之间，粗排需要更多获取和利用后续链路的信息来提升效果。
* **性能约束**：线上粗排预测的候选集远远高于精排模型，然而实际整个搜索系统对性能有严格的要求，导致粗排需要重点关注预测性能。


本文将围绕上述挑战点来分享美团搜索粗排层优化的相关探索与实践，其中样本选择偏差问题我们放在精排联动问题中一起解决。本文主要分成三个部分：第一部分会简单介绍美团搜索排序粗排层的演进路线；第二部分介绍粗排优化的相关探索与实践，其中第一个工作是采用知识蒸馏和对比学习使精排和粗排联动来优化粗排效果，第二个工作是考虑粗排性能和效果 trade\-off 的粗排优化，相关工作均已全量上线，且效果显著；最后是总结与展望部分，希望这些内容对大家有所帮助和启发。



## 2. 粗排演进路线


美团搜索的粗排技术演进分为以下几个阶段：



* 2016 年：基于相关性、质量度、转化率等信息进行线性加权，这种方法简单但是特征的表达能力较弱，权重人工确定，排序效果存在很大的提升空间。
* 2017 年：采用基于机器学习的简单 LR 模型进行 Pointwise 预估排序。
* 2018 年：采用基于向量内积的双塔模型，两侧分别输入查询词、用户以及上下文特征和商户特征，经过深度网络计算后，分别产出用户&查询词向量和商户向量，再通过内积计算得到预估分数进行排序。该方法可以提前把商户向量计算保存好，所以在线预测快，但是两侧信息的交叉能力有限。
* 2019 年：为了解决双塔模型无法很好地建模交叉特征的问题，将双塔模型的输出作为特征与其他交叉特征通过 GBDT 树模型进行融合。
* 2020 年至今：由于算力的提升，开始探索 NN 端到端粗排模型并且持续迭代 NN 模型。


现阶段，工业界粗排模型常用的有双塔模型，比如腾讯\[3\]和爱奇艺\[4\]；交互式 NN 模型，比如阿里巴巴\[1,2\]。下文主要介绍美团搜索在粗排升级为 NN 模型过程中的相关优化工作，主要包括粗排效果优化、效果&性能联合优化两个部分。



## 3. 粗排优化实践


随着大量的效果优化工作\[5,6\]在美团搜索精排 NN 模型落地，我们也开始探索粗排 NN 模型的优化。考虑到粗排有严格的性能约束，直接将精排优化的工作复用到粗排是不适用的。下面会介绍关于将精排的排序能力迁移到粗排的精排联动效果优化工作，以及基于神经网络结构自动搜索的效果和性能 trade\-off 优化工作。



### 3.1 精排联动效果优化


粗排模型受限于打分性能约束，这会导致模型结构相比精排模型更加简单，特征数量也比精排少很多，因此排序效果要差于精排。为了弥补粗排模型结构简单、特征较少带来的效果损失，我们尝试知识蒸馏方法\[7\]来联动精排对粗排进行优化。



知识蒸馏是目前业界简化模型结构并最小化效果损失的普遍方法，它采取一种 Teacher\-Student 范式：结构复杂、学习能力强的模型作为 Teacher 模型，结构较为简单的模型作为 Student 模型，通过 Teacher 模型来辅助 Student 模型训练，从而将 Teacher 模型的“知识”传递给 Student 模型，实现 Student 模型的效果提升。精排蒸馏粗排的示意图如下图 2 所示，蒸馏方案分为以下三种：精排结果蒸馏、精排预测分数蒸馏、特征表征蒸馏。下面会分别介绍这些蒸馏方案在美团搜索粗排中的实践经验。



![图2 精排蒸馏粗排示意图](https://p1.meituan.net/travelcube/65f3f283eacf1193f4a6b069cefcecdf271853.png)



#### 3.1.1 精排结果列表蒸馏


粗排作为精排的前置模块，它的目标是初步筛选出质量比较好的候选集合进入精排，从训练样本选取来看，除了常规的用户发生行为（点击、下单、支付）的 item 作为正样本，曝光未发生行为的 item 作为负样本外，还可以引入一些通过精排模型排序结果构造的正负样本，这样既能一定程度缓解粗排模型的样本选择偏置，也能将精排的排序能力迁移到粗排。下面会介绍在美团搜索场景下，使用精排排序结果蒸馏粗排模型的实践经验。



**策略1**：在用户反馈的正负样本基础上，随机选取少量精排排序靠后的未曝光样本作为粗排负样本的补充，如图 3 所示。该项改动离线 Recall@150（指标解释参看附录）\+5PP，线上 CTR \+0.1%。



![图3 补充排序结果靠后负例](https://p1.meituan.net/travelcube/4a3fd85589c1882bdcd84376203cf58e150014.png)



**策略2**：直接在精排排序后的集合里面进行随机采样得到训练样本，精排排序的位置作为 label 构造 pair 对进行训练，如下图 4 所示。离线效果相比策略1 Recall@150 \+2PP，线上 CTR \+0.06%。



![图4 排序靠前靠后构成 pair 对样本](https://p0.meituan.net/travelcube/60fb9f9eb7d78a6748093d08be09964d201430.png)



**策略3**：基于策略2的样本集选取，采用对精排排序位置进行分档构造 label ，然后根据分档 label 构造 pair 对进行训练。离线效果相比策略2 Recall@150 \+3PP，线上 CTR \+0.1%。



#### 3.1.2 精排预测分数蒸馏


前面使用排序结果蒸馏是一种比较粗糙使用精排信息的方式，我们在这个基础上进一步添加预测分数蒸馏\[8\]，希望粗排模型输出的分数与精排模型输出的分数分布尽量对齐，如下图 5 所示：



![图5 精排预测分数构造辅助损失](https://p0.meituan.net/travelcube/4d4e0d6db11ae44e516f24d8eb2abd28189647.png)



在具体实现上，我们采用两阶段蒸馏范式，基于预先训练好的精排模型来蒸馏粗排模型，蒸馏 Loss 采用的是粗排模型输出和精排模型输出的最小平方误差，并且添加一个参数 Lambda 来控制蒸馏 Loss 对最终 Loss 的影响，如公式（1）所示。 使用精排分数蒸馏的方法，离线效果 Recall@150 \+5PP，线上效果 CTR \+0.05%。



![](https://p0.meituan.net/travelcube/1d8aa6ab40481c1237029699ccec697c14029.png)



#### 3.1.3 特征表征蒸馏


业界通过知识蒸馏实现精排指导粗排表征建模已经被验证是一种有效提升模型效果的方式\[7\]，然而直接用传统的方法蒸馏表征有以下缺陷：第一是无法蒸馏粗排和精排之间的排序关系，而前文已提到，排序结果蒸馏在我们的场景中，线下、线上均有效果提升；第二是传统采用 KL 散度作为表征度量的知识蒸馏方案，把表征的每一维独立对待，无法有效地蒸馏高度相关的、结构化的信息\[9\]，而在美团搜索场景下，数据是高度结构化的，因此采用传统的知识蒸馏策略来做表征蒸馏可能无法较好地捕获这种结构化的知识。



![](https://p0.meituan.net/travelcube/4f9e6904232636262232ca90bedd9940292267.png)



![图6 对比学习精排信息迁移](https://p0.meituan.net/travelcube/76b10e9fbd1e39da31a65ec2cd51cfe1475998.png)



在上文公式 \(1\) 的基础上，补充对比学习表征蒸馏 Loss，离线效果 Recall@150 \+14PP，线上 CTR \+0.15%。相关工作的详细内容可以参考我们的论文\[10\]（正在投稿中）。



![](https://p0.meituan.net/travelcube/5024fae3f94b2722081d86349e87abad16521.png)



### 3.2 效果性能联合优化


前面提到线上预测的粗排候选集较大，考虑到系统全链路性能的约束，粗排需要考虑预测效率。前文提到的工作都是基于简单 DNN \+ 蒸馏的范式来进行优化，但是存在如下两个问题：



* 目前受限于线上性能而只使用了简单特征，未引入更加丰富的交叉特征，导致模型效果还有进一步提升的空间。
* 固定粗排模型结构的蒸馏会损失蒸馏效果，从而造成次优解\[11\]。


根据我们的实践经验，直接在粗排层引入交叉特征是不能满足线上时延要求的。因此为了解决以上问题，我们探索并实践了基于神经网络架构搜索的粗排建模方案，该方案同时优化粗排模型的效果和性能，选择出满足粗排时延要求的最佳特征组合和模型结构，整体架构图如下图7所示：



![图7 基于 NAS 的特征和模型结构选择](https://p0.meituan.net/travelcube/f752440fe78ab057e9750ed732021154959272.png)



下面我们对其中的神经网络架构搜索（NAS）以及引入效率建模这两个关键技术点进行简单介绍：



![](https://p0.meituan.net/travelcube/2010fe0380e405e845b9c7af360fc161482501.png)



![图8 模型延时计算图](https://p1.meituan.net/travelcube/88286a9e0d397e252d4201245cd37b6a392388.png)



![](https://p1.meituan.net/travelcube/a237430d2ff8f17b6b7ed948b1143456126626.png)



![](https://p0.meituan.net/travelcube/d2522cc6b80f4d6e1fa3cfde86501af2270658.png)



通过神经网络架构搜索的建模来联合优化粗排模型的效果和预测性能，离线 Recall@150 \+11PP， 最终在线上延时不增加的情况下，线上指标 CTR \+0.12%；详细工作可参考\[13\]，已被 KDD 2022 接收。



## 4. 总结


从 2020 年开始，我们通过大量的工程性能优化使粗排层落地 MLP 模型，在2021 年我们继续在 MLP 模型基础上，持续迭代粗排模型来提升粗排效果。首先，我们借鉴业界常用的蒸馏方案来联动精排优化粗排，从精排结果蒸馏、精排预测分数蒸馏、特征表征蒸馏三个层面分别进行了大量实验，在不增加线上延时的情况下，提升粗排模型效果。



其次，考虑到传统蒸馏方式无法很好处理排序场景中的特征结构化信息，我们自研了一套基于对比学习的精排信息迁移粗排方案。



最后，我们进一步考虑到粗排优化本质上是效果和性能的 trade\-off，采用多目标建模的思路同时优化效果和性能，落地神经网络架构自动搜索技术来进行求解，让模型自动选择效率和效果最佳的特征集合和模型结构。后续我们会从以下几个方面继续迭代粗排层技术：



* **粗排多目标建模**：目前的粗排本质上还是一个单目标模型，目前我们正在尝试将精排层的多目标建模应用于粗排。
* **粗排联动的全系统动态算力分配**：粗排可以控制召回的算力以及精排的算力，针对不同场景，模型需要的算力是不一样的，因此动态算力分配可以在不降低线上效果的情况下减小系统算力消耗，目前我们已经在这个方面取得了一定的线上效果。


## 5. 附录


传统的排序离线指标多以 NDCG、MAP、AUC 类指标为标准，对于粗排来说，其本质更偏向于以集合选择为目标的召回类任务，因此传统的排序指标不利于衡量粗排模型迭代效果好坏。我们借鉴\[6\]中 Recall 指标作为粗排离线效果的衡量指标，即以精排排序结果为 ground truth，衡量粗排和精排排序结果 TopK 的对齐程度。Recall 指标具体定义如下：



![](https://p0.meituan.net/travelcube/d4058b66afbe7261a1ea52eac5a7b16d292194.png)



该公式的物理含义即为衡量粗排排序前 K 个和精排排序前 K 的重合度，该指标更加符合粗排集合选择的本质。



## 6. 作者简介


晓江、所贵、李想、曹越、培浩、肖垚、达遥、陈胜、云森、利前等，均来自美团平台/搜索推荐算法部。



## 7. 参考文献


* \[1\] Wang Z, Zhao L, Jiang B, et al. Cold: Towards the next generation of pre\-ranking system\[J\]. arXiv preprint arXiv:2007.16122, 2020.
* \[2\] Ma X, Wang P, Zhao H, et al. Towards a Better Tradeoff between Effectiveness and Efficiency in Pre\-Ranking: A Learnable Feature Selection based Approach\[C\]//Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021: 2036\-2040.
* \[3\] [https://mp.weixin.qq.com/s/Jfuc6x\-Qt0rya5dbCR2uCA](https://mp.weixin.qq.com/s/Jfuc6x-Qt0rya5dbCR2uCA)
* \[4\] [https://mp.weixin.qq.com/s/RwWuZBSaoVXVmZpnyg7FHg](https://mp.weixin.qq.com/s/RwWuZBSaoVXVmZpnyg7FHg)
* \[5\] [https://tech.meituan.com/2020/04/16/transformer\-in\-meituan.html](https://tech.meituan.com/2020/04/16/transformer-in-meituan.html).
* \[6\] [https://tech.meituan.com/2021/07/08/multi\-business\-modeling.html](https://tech.meituan.com/2021/07/08/multi-business-modeling.html).
* \[7\] Tang, Jiaxi, and Ke Wang. “Ranking distillation: Learning compact ranking models with high performance for recommender system.” Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2018.
* \[8\] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 \(2015\).
* \[9\] Chen L, Wang D, Gan Z, et al. Wasserstein contrastive representation distillation\[C\]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 16296\-16305.
* \[10\] [https://arxiv.org/abs/2207.03073](https://arxiv.org/abs/2207.03073)
* \[11\] Liu Y, Jia X, Tan M, et al. Search to distill: Pearls are everywhere but not the eyes\[C\]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 7539\-7548.
* \[12\] Cai H, Zhu L, Han S. Proxylessnas: Direct neural architecture search on target task and hardware\[J\]. arXiv preprint arXiv:1812.00332, 2018.
* \[13\] [https://arxiv.org/abs/2205.09394](https://arxiv.org/abs/2205.09394)


## 招聘信息


搜索推荐算法部/基础算法组是负责美团搜索研发的核心团队，使命是打造世界一流的搜索引擎，依托Deep Learning（深度学习）、NLP（自然语言处理）、Knowledge Graph（知识图谱）等技术，处理美团海量用户、商家、商品数据，不断加深对用户、场景、查询和服务的理解，高效地支撑形态各样的生活服务搜索，解决搜索结果的多业务混排、相关性、个性化等问题，给用户极致的搜索体验。搜索推荐算法部长期招聘搜索推荐算法专家，感兴趣的同学可以将简历发送至：tech@meituan.com（邮件主题：美团平台/搜索推荐算法部）。





