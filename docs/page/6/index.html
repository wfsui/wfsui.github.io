<!DOCTYPE html>
<html>
<head>
	<meta name="generator" content="Hugo 0.117.0">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>大峰哥 - 记录日常生活哦 </title><meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="alternate" type="application/rss+xml" href="https://wfsui.github.io/index.xml" title="大峰哥" />
	<meta property="og:title" content="大峰哥" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://wfsui.github.io/" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="大峰哥"/>
<meta name="twitter:description" content=""/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://wfsui.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://wfsui.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://wfsui.github.io/css/dark.css" media="(prefers-color-scheme: dark)" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="https://wfsui.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper">
		<div class="header">
	<base href="https://wfsui.github.io/">
	<h1 class="site-title"><a href="https://wfsui.github.io/">大峰哥</a></h1>
	<div class="site-description"><h2>记录日常生活哦</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/wfsui" title="Github"><i data-feather="github"></i></a><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">首页</a>
			</li>
			
			<li>
				<a href="/posts">文章</a>
			</li>
			
			<li>
				<a href="/about">关于</a>
			</li>
			
			<li>
				<a href="/tags">标签</a>
			</li>
			
		</ul>
	</nav>
</div>


		

		<div class="recent-posts section">
			<h2 class="section-header">
				Recent posts
			</h2>
			<div class="posts">
				
				
				
				<div class="post">
					<div class="meta">Aug 10, 2023</div>
					<a class="title" href="/posts/%E7%BE%8E%E5%9B%A2%E8%A7%86%E8%A7%89gpu%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/">美团视觉GPU推理服务部署架构优化实践</a> &mdash;
					<span class="description">
						
						0. 导读 美团视觉面向本地生活服务，在众多场景上落地应用了文字识别、图像质量评价、视频理解等视觉AI技术。此前，在线推理服务使用的GPU资源不断增加，但服务GPU利用率普遍较低，浪费大量计算资源，增加了视觉AI应用成本，这是美团也是很多企业亟需解决的问题。
美团视觉智能部通过实验分析发现，造成视觉推理服务GPU利用率低下的一个重要原因是模型结构问题：模型中预处理或者后处理部分CPU运算速度慢，导致推理主干网络无法充分发挥GPU运算性能。基于此，视觉研发团队通过模型结构拆分和微服务化，提出一种通用高效的部署架构，解决这种常见的性能瓶颈问题。
目前，该解决方案已经在多个核心服务上成功应用。以“图像检测+分类”服务为例，优化后的服务压测性能指标GPU利用率由40%提升至100%，QPS提升超过3倍。本文将会重点介绍推理服务部署架构优化的工程实践，希望对从事相关工作的同学们有所帮助或启发。
1. 背景 随着越来越多的AI应用进入生产应用阶段，推理服务所需要的GPU资源也在迅速增加。调研数据表明，国内AI相关行业推理服务的资源使用量占比已经超过55%，且比例未来还会持续升高。但很多公司面临的实际问题是，线上推理服务GPU利用率普遍较低，还具备很大的提升空间。
而造成服务GPU利用率低的重要原因之一是：推理服务本身存在性能瓶颈，在极限压力情况下也无法充分利用GPU资源。在这种背景下，“优化推理服务性能、提高GPU资源使用效率、降低资源使用成本”具有非常重要的意义。本文主要介绍如何通过架构部署优化，在保证准确率、推理时延等指标的前提下，提升推理服务的性能和GPU利用率。
2. 视觉模型服务的特点与挑战 近年来，深度学习方法在计算机视觉任务上取得显著进展，已经成为主流方法。视觉模型在结构上具有一些特殊性，如果使用现有推理框架部署，服务在性能和GPU利用率方面可能无法满足要求。
2.1 模型优化工具与部署框架 深度学习模型部署前通常会使用优化工具进行优化，常见的优化工具包括TensorRT、TF-TRT、TVM和OpenVINO等。这些工具通过算子融合、动态显存分配和精度校准等方法提高模型运行速度。模型部署是生产应用的最后一环，它将深度学习模型推理过程封装成服务，内部实现模型加载、模型版本管理、批处理以及服务接口封装等功能，对外提供RPC/HTTP接口。业界主流的部署框架有以下几种：
TensorFlow Serving：TensorFlow Serving（简称TF-Serving）是Google发布用于机器学习模型部署的高性能开源框架，内部集成了TF-TRT优化工具，但是对非TensorFlow格式的模型支持不够友好。 Torch Serve：TorchServe是AWS和Facebook联合推出的Pytorch模型部署推理框架，具有部署简单、高性能、轻量化等优点。 Triton：Triton是Nvidia发布的高性能推理服务框架，支持TensorFlow、TensorRT、PyTorch和ONNX等多种框架模型，适用于多模型联合推理场景。 在实际部署中，无论选择哪种框架，需要同时考虑模型格式、优化工具、框架功能特点等多种因素。&hellip;
						
					</span>
				</div>
				
				<div class="post">
					<div class="meta">Aug 3, 2023</div>
					<a class="title" href="/posts/code%E7%BE%8E%E5%9B%A2%E4%BB%A3%E7%A0%81%E6%89%98%E7%AE%A1%E5%B9%B3%E5%8F%B0%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B8%8E%E5%AE%9E%E8%B7%B5/">Code：美团代码托管平台的演进与实践</a> &mdash;
					<span class="description">
						
						1. 引言 Code是美团自研的代码托管平台，其中包括了代码版本管理、分支管理及代码评审等功能，协同众多研发流程工具平台，支撑内部所有工程师的日常研发工作。经过近3年的建设，目前Code托管了数以万计的仓库，日常处理千万级的Git相关请求，稳定支撑着美团研发流程规范的持续落地。本文主要介绍美团在建设代码托管平台过程中面临的一些挑战和实践经验。
2. 美团代码托管平台建设之路 2.1 代码托管平台的发展史 回顾美团代码托管平台的发展史，整个历程可以划分为三个阶段：单机部署、多机部署以及自研分布式代码托管平台。
第一阶段：单机部署 美团最初的代码托管平台，和绝大多数Web系统一样，单机部署即可运行，所有用户的请求均通过Web应用进行响应。由于Git使用基于文件组织形式的存储模式，无论是通过页面访问还是执行Git命令操作，最终都会表现为磁盘的文件读写，高IO磁盘尤为重要。整体架构如下图1所示：
第二阶段：多机部署 在访问规模不大的情况下，第一阶段这种单机架构可以满足日常的开发需求。但随着研发团队业务需求的不断增长，测试自动化流程的逐步完善，扩展性瓶颈也愈发明显，主要表现为以下2个方面：
存储：由于公司资源限制和地域分配不均等因素，代码托管平台部署机器已配置最大容量的可用SSD磁盘，使用率仍高达80%，可用空间严重不足。 负载：随着研发人员的不断增多，在访问高峰期，CPU和IO负载高达95%以上，页面出现严重的卡顿，仅能通过限流保障系统的持续服务。 因而，单机部署无法再承载高峰期的访问量，系统扩容刻不容缓。于是，我们开始设计了一套能够通过多机负载同一仓库IO的读写分离架构方案，以解决较为严重的IO负载问题。在读写分离架构中，最重要的是要保证用户视角的数据一致性（用户随时可以读取提交的最新代码），这里采取了以下措施：
写操作仅发生在主节点。 采用懒汉同步模式，在读取数据时触发从节点同步数据，若失败，则路由到主节点。 采用独主兜底模式，遇遇到突发情况时可以迅速禁用从节点，保障数据安全。 如图2所示，我们将仓库访问形式按照应用层协议区分为HTTP和SSH，分别由对应的解析代理模块进行读写分发操作后再下发到主从节点（此处采用了Round-Bobin的算法分发读请求），使得读吞吐量整体扩大了2倍。对于从节点，我们部署了Agent，在用户发起读请求时会触发同步仓库数据的Fetch操作，以保证数据的一致性。
第三阶段：自研分布式代码托管平台 在第二阶段，虽然通过多机负载IO的读写分离架构短暂性地解决了扩展性瓶颈问题，但仓库数据仍在持续不断地指数增长。同时，除扩展性问题之外，可用性瓶颈也凸显出来，主要表现在以下2个方面：&hellip;
						
					</span>
				</div>
				
				<div class="post">
					<div class="meta">Aug 1, 2023</div>
					<a class="title" href="/posts/%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E5%B9%B4%E8%B4%A7%E6%9D%A5%E4%BA%861300&#43;%E9%A1%B5%E7%9A%84%E7%94%B5%E5%AD%90%E4%B9%A6%E6%B6%B5%E7%9B%96%E5%89%8D%E7%AB%AF%E5%90%8E%E7%AB%AF%E7%AE%97%E6%B3%95%E6%95%B0%E6%8D%AE%E8%BF%90%E7%BB%B4%E5%AE%89%E5%85%A8/">美团技术年货来了！1300&#43;页的电子书，涵盖前端、后端、算法、数据、运维、安全</a> &mdash;
					<span class="description">
						
						新春将至，一年一度的美团技术年货也如约到来。
时间煮雨，岁月缝花，花开无声，花谢无语。2022这一年，我们一起经历了无数的悲喜，也留下了满满的回忆。
也许生活就是这样，只有历尽波澜，才能欣赏茫茫大海的辽阔和无边，才能感受到漫天星辰的光芒和温暖。
在2023年春节到来之际，我们从去年美团技术团队公众号上精选了60多篇技术文章，整理制作成一本1300多页的电子书，作为新年礼物赠送给大家。
这本电子书内容覆盖算法、前端、后端、数据、运维/安全等多个技术领域，希望能对同学们的工作和学习有所帮助。
也欢迎大家转给更多有相同兴趣、爱好学习的同事和朋友们，一起切磋，共同成长。
祝愿2023年，大家诸事顺遂，健康平安。
如何获取？ 回复关键字【2022算法】，下载20022年算法系列（共430页，约34M） 回复关键字【2022前端】，下载2022年前端系列（共198页，约16M） 回复关键字【2022后端】，下载2022年后端系列（共575页，约24M） 回复关键字【2022综合】，下载2022年数据·运维·安全系列（共160页，约6M） 温馨提示：
美团技术年货合集大小约为48M，下载需要一些时间； 打开电子书目录后，可直接点击感兴趣的标题进行阅读； 部分文章中的动态图片无法在电子书中进行完全的展示，大家可以移步美团技术团队官方博客 tech.meituan.com ，或在美团技术团队公众号历史文章中进行阅读，感谢理解。 往期技术年货下载 关注「美团技术团队」微信公众号。回复【2021年货】、【2020年货】、【2019年货】、 【2018年货】、【2017年货】，即可获取往期年货下载链接。&hellip;
						
					</span>
				</div>
				
				<div class="post">
					<div class="meta">Jul 19, 2023</div>
					<a class="title" href="/posts/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0sdk%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E8%B7%B5/">美团开放平台SDK自动生成技术与实践</a> &mdash;
					<span class="description">
						
						1. 引言 美团开放平台对外提供了外卖、团购、配送等20余个业务场景的OpenAPI，供第三方开发者搭建应用时使用，是美团系统与外部系统通讯的最重要平台。本文主要讲述开放平台如何通过技术手段自动生成支持接口参数富模型和多种编程语言的SDK，以提高开发者对接开放平台API的效率。
1.1 背景 美团开放平台将美团各类业务提供的扩展服务封装成一系列应用程序编程接口（API）对外开放，供第三方开发者使用。开发者可通过调用开放平台提供的OpenAPI获取数据和能力，以实现自身系统与美团系统协同工作的业务逻辑。以外卖业务场景为例，开发者可以在自己为外卖商户开发的应用中通过调用美团开放平台提供的API，提供外卖订单查询、接单、订单管理等一系列功能。如下图所示：
开放平台为开发者提供的OpenAPI以HTTP接口的形式提供。以平台提供的订单查询接口为例，对应的HTTP请求如下所示：
POST https://api-open-cater.meituan.com/api/order/queryById Content-Type: application/x-www-form-urlencoded;charset=utf-8 appAuthToken=eeee860a3d2a8b73cfb6604b136d6734283510c4e92282&amp; charset=utf-8&amp; developerId=106158&amp; sign=4656285a4c2493e279d929b8b9f4e29310da8b2b&amp; timestamp=1618543567&amp; biz={&#34;orderId&#34;: &#34;10046789912119&#34;} Response:{ &#34;orderId&#34;:&#34;10046789912119&#34;, &#34;payAmount&#34;:&#34;45.&hellip;
						
					</span>
				</div>
				
				<div class="post">
					<div class="meta">Jul 17, 2023</div>
					<a class="title" href="/posts/%E4%BA%BA%E6%9C%BA%E8%AF%AD%E9%9F%B3%E4%BA%A4%E4%BA%92%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E4%BD%8E%E5%BB%B6%E8%BF%9F%E6%B5%81%E5%BC%8F%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%8A%80%E6%9C%AF/">人机语音交互场景中的低延迟流式语音识别技术</a> &mdash;
					<span class="description">
						
						1. 前言 1.1 语音识别技术简介 人机交互一直都是人工智能大背景下的“热门话题”，语音交互作为人机交互的一个重要分支，具有广泛的应用价值，也被应用到美团的多个业务场景中，如智能客服、电话营销和电话满意度反馈等。而流式语音识别技术是整个交互链条的入口，对交互体验影响巨大。
常见的语音识别大多都是非流式语音识别技术，它是指模型在用户说完一句话或一段话之后再进行识别。这意味着模型需要等待用户停顿或结束说话才能开始识别，并且只能在用户停顿或结束说话后才能输出完整的识别结果。这样做的缺点是会导致较长的延迟和不连贯的交互。例如，在会议场景中，如果使用非流式语音识别技术，就可能出现会议参与者说了很长时间的话才显示出他们所说的内容，而且可能因为网络延迟或其他原因导致内容显示不全或错误。这样就会影响会议参与者之间的沟通和理解，并且降低会议效率和质量。
而与之对应的是流式语音识别技术，它是指可以在处理音频流的过程中，支持实时返回识别结果的一类语音识别模型。这意味着模型不需要等待用户说完整句或整段话就可以开始识别，并且可以随着用户说话的进度逐渐输出识别结果。这样做的好处是能够大大减少人机交互过程中语音识别的处理时间，提高用户体验和交互效率。例如，在智能客服场景中，使用流式语音识别技术，就可以实现用户说一句话很快就能获得机器人响应，而不是等到用户说完一段话才给出回答。这样就可以让用户更快地得到满意的解决方案，并且减少用户的等待时间和不满情绪，提升用户满意度。在美团内部的众多业务场景中广泛使用了流式语音识别技术。
本文将详细阐述团队在语音交互场景中的低延迟流式语音识别方案，目前以该方案形成的技术论文《Peak-First CTC: Reducing the Peak Latency of CTC Models by Applying Peak-First Regularization》已经被语音领域国际顶级会议ICASSP 2023收录。&hellip;
						
					</span>
				</div>
				
				

<ul class="pagination">
	<li class="page-item page-prev">
	
    <a href="/page/5/" class="page-link" aria-label="Previous"><span aria-hidden="true">← Prev page</span></a>
	
	</li>
	<li class="page-item page-next">
	
    <a href="/page/7/" class="page-link" aria-label="Next"><span aria-hidden="true">Next page →</span></a>
	
	</li>
</ul>


			</div>
		</div>
	</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> © Copyright notice |  <a href="">Wfsui theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div><script>feather.replace()</script>
</body>
</html>
