<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>低延迟流式语音识别技术在人机语音交互场景中的实践 - 大峰哥</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="低延迟流式语音识别技术在人机语音交互场景中的实践" />
<meta property="og:description" content="1. 前言 1.1 语音识别技术简介 人机交互一直都是人工智能大背景下的“热门话题”，语音交互作为人机交互的一个重要分支，具有广泛的应用价值，也被应用到美团的多个业务场景中，如智能客服、电话营销和电话满意度反馈等。而流式语音识别技术是整个交互链条的入口，对交互体验影响巨大。
常见的语音识别大多都是非流式语音识别技术，它是指模型在用户说完一句话或一段话之后再进行识别。这意味着模型需要等待用户停顿或结束说话才能开始识别，并且只能在用户停顿或结束说话后才能输出完整的识别结果。这样做的缺点是会导致较长的延迟和不连贯的交互。例如，在会议场景中，如果使用非流式语音识别技术，就可能出现会议参与者说了很长时间的话才显示出他们所说的内容，而且可能因为网络延迟或其他原因导致内容显示不全或错误。这样就会影响会议参与者之间的沟通和理解，并且降低会议效率和质量。
而与之对应的是流式语音识别技术，它是指可以在处理音频流的过程中，支持实时返回识别结果的一类语音识别模型。这意味着模型不需要等待用户说完整句或整段话就可以开始识别，并且可以随着用户说话的进度逐渐输出识别结果。这样做的好处是能够大大减少人机交互过程中语音识别的处理时间，提高用户体验和交互效率。例如，在智能客服场景中，使用流式语音识别技术，就可以实现用户说一句话很快就能获得机器人响应，而不是等到用户说完一段话才给出回答。这样就可以让用户更快地得到满意的解决方案，并且减少用户的等待时间和不满情绪，提升用户满意度。在美团内部的众多业务场景中广泛使用了流式语音识别技术。
本文将详细阐述团队在语音交互场景中的低延迟流式语音识别方案，目前以该方案形成的技术论文《Peak-First CTC: Reducing the Peak Latency of CTC Models by Applying Peak-First Regularization》已经被语音领域国际顶级会议ICASSP 2023收录。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wfsui.github.io/posts/%E4%BD%8E%E5%BB%B6%E8%BF%9F%E6%B5%81%E5%BC%8F%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%8A%80%E6%9C%AF%E5%9C%A8%E4%BA%BA%E6%9C%BA%E8%AF%AD%E9%9F%B3%E4%BA%A4%E4%BA%92%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E5%AE%9E%E8%B7%B5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-30T02:40:10+00:00" />
<meta property="article:modified_time" content="2023-09-30T02:40:10+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="低延迟流式语音识别技术在人机语音交互场景中的实践"/>
<meta name="twitter:description" content="1. 前言 1.1 语音识别技术简介 人机交互一直都是人工智能大背景下的“热门话题”，语音交互作为人机交互的一个重要分支，具有广泛的应用价值，也被应用到美团的多个业务场景中，如智能客服、电话营销和电话满意度反馈等。而流式语音识别技术是整个交互链条的入口，对交互体验影响巨大。
常见的语音识别大多都是非流式语音识别技术，它是指模型在用户说完一句话或一段话之后再进行识别。这意味着模型需要等待用户停顿或结束说话才能开始识别，并且只能在用户停顿或结束说话后才能输出完整的识别结果。这样做的缺点是会导致较长的延迟和不连贯的交互。例如，在会议场景中，如果使用非流式语音识别技术，就可能出现会议参与者说了很长时间的话才显示出他们所说的内容，而且可能因为网络延迟或其他原因导致内容显示不全或错误。这样就会影响会议参与者之间的沟通和理解，并且降低会议效率和质量。
而与之对应的是流式语音识别技术，它是指可以在处理音频流的过程中，支持实时返回识别结果的一类语音识别模型。这意味着模型不需要等待用户说完整句或整段话就可以开始识别，并且可以随着用户说话的进度逐渐输出识别结果。这样做的好处是能够大大减少人机交互过程中语音识别的处理时间，提高用户体验和交互效率。例如，在智能客服场景中，使用流式语音识别技术，就可以实现用户说一句话很快就能获得机器人响应，而不是等到用户说完一段话才给出回答。这样就可以让用户更快地得到满意的解决方案，并且减少用户的等待时间和不满情绪，提升用户满意度。在美团内部的众多业务场景中广泛使用了流式语音识别技术。
本文将详细阐述团队在语音交互场景中的低延迟流式语音识别方案，目前以该方案形成的技术论文《Peak-First CTC: Reducing the Peak Latency of CTC Models by Applying Peak-First Regularization》已经被语音领域国际顶级会议ICASSP 2023收录。"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://wfsui.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://wfsui.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://wfsui.github.io/css/dark.css" media="(prefers-color-scheme: dark)" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="https://wfsui.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<base href="https://wfsui.github.io/">
	<h1 class="site-title"><a href="https://wfsui.github.io/">大峰哥</a></h1>
	<div class="site-description"><h2>记录日常生活哦</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/wfsui" title="Github"><i data-feather="github"></i></a><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">首页</a>
			</li>
			
			<li>
				<a href="/posts">文章</a>
			</li>
			
			<li>
				<a href="/about">关于</a>
			</li>
			
			<li>
				<a href="/tags">标签</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">低延迟流式语音识别技术在人机语音交互场景中的实践</h1>
			<div class="meta">Posted at &mdash; Sep 30, 2023</div>
		</div>

		<div class="markdown">
			<h2 id="1-前言">1. 前言</h2>
<h3 id="11-语音识别技术简介">1.1 语音识别技术简介</h3>
<p>人机交互一直都是人工智能大背景下的“热门话题”，语音交互作为人机交互的一个重要分支，具有广泛的应用价值，也被应用到美团的多个业务场景中，如智能客服、电话营销和电话满意度反馈等。而流式语音识别技术是整个交互链条的入口，对交互体验影响巨大。</p>
<p>常见的语音识别大多都是非流式语音识别技术，它是指模型在用户说完一句话或一段话之后再进行识别。这意味着模型需要等待用户停顿或结束说话才能开始识别，并且只能在用户停顿或结束说话后才能输出完整的识别结果。这样做的缺点是会导致较长的延迟和不连贯的交互。例如，在会议场景中，如果使用非流式语音识别技术，就可能出现会议参与者说了很长时间的话才显示出他们所说的内容，而且可能因为网络延迟或其他原因导致内容显示不全或错误。这样就会影响会议参与者之间的沟通和理解，并且降低会议效率和质量。</p>
<p>而与之对应的是流式语音识别技术，它是指可以在处理音频流的过程中，支持实时返回识别结果的一类语音识别模型。这意味着模型不需要等待用户说完整句或整段话就可以开始识别，并且可以随着用户说话的进度逐渐输出识别结果。这样做的好处是能够大大减少人机交互过程中语音识别的处理时间，提高用户体验和交互效率。例如，在智能客服场景中，使用流式语音识别技术，就可以实现用户说一句话很快就能获得机器人响应，而不是等到用户说完一段话才给出回答。这样就可以让用户更快地得到满意的解决方案，并且减少用户的等待时间和不满情绪，提升用户满意度。在美团内部的众多业务场景中广泛使用了流式语音识别技术。</p>
<p>本文将详细阐述团队在语音交互场景中的低延迟流式语音识别方案，目前以该方案形成的技术论文《<a href="https://arxiv.org/abs/2211.03284">Peak-First CTC: Reducing the Peak Latency of CTC Models by Applying Peak-First Regularization</a>》已经被语音领域国际顶级会议<a href="https://2023.ieeeicassp.org/">ICASSP 2023</a>收录。</p>
<h3 id="12-问题与挑战">1.2 问题与挑战</h3>
<p>对一个好的流式语音识别系统而言，不仅仅需要高的识别准确率，还应该具有很低的延迟。在交互场景中，低延迟可以提高用户体验和满意度，让用户感觉不到语音识别的处理时间，更加自然和流畅地进行对话和问答。低延迟也可以减少通话交流中的误解和冲突，让用户能够及时地收到反馈结果，避免重复或打断对方的说话。此外，低延迟还可以增加语音应用的可用性和灵活性，让用户能够在各种场景下通过说话来完成任务（例如在线游戏、语音助手、智能家居等），节省下来的延迟也可以用于在语音服务的上下游部署更加复杂的模型结构，从而进一步完善交互链路等。</p>
<p>在美团的交互场景中，广泛使用联结时序分类模型（Connectionist Temporal Classification， CTC ）作为基础模型来构架流式语音识别系统。CTC 模型由于其优雅的模型结构、卓越的模型表现以及良好的扩展性受到了广泛的青睐。目前已经广泛应用在语音识别（Automatic Speech Recognition, ASR）、语音翻译（Speech Translation, ST） 以及 光学字符识别（Optical Character Recognition, OCR）等领域。</p>
<p>下图展示了一种典型的 CTC 模型结构，其依赖 DFSMN 网络结构搭建，仅包含声学编码器（Acoustic Encoder）和输出线性映射层两部分。声学编码器用来将输出的声学特征序列转变成声学编码序列，而输出线性映射层则负责将利用声学编码表示，计算得到模型预测出不同文本标记的概率值。对比其他流式语音识别模型，CTC 模型不需要复杂的编码解码（Encoder-Decoder）结构或者注意力机制（Attention Mechanism）就能实现两个不等长序列之间的转换（对于语音识别而言是从声学特征序列转换到目标文本序列）。</p>
<p><img src="https://p0.meituan.net/travelcube/1fc2d2de82f3bbe7460ed0d173308516194783.png" alt="基于 DFSMN-CTC 的语音识别模型结构"></p>
<p>基于 CTC 的流式语音识别系统对于延迟也有着非常高的要求。从用户发音结束到系统识别出对应文字之间的时间差被称之为出字延迟。出字延迟越低则意味着 ASR 系统吐字的速度越快，用户体验越好。下图展示了 CTC 模型的输出概率分布，其中顶部色块表示用户说的每个文本的发声范围，而底部对应颜色的尖峰则表示系统识别出的文本所在的位置。出字延迟则对应着色块尾部与概率尖峰位置之间的时间差。本文所展示的工作就聚焦于如何降流式 CTC 语音识别系统的出字延迟。</p>
<p><img src="https://p1.meituan.net/travelcube/2bb632474abb1073300d194bf6ab0201369018.png" alt="CTC模型的输出概率分布"></p>
<h2 id="2-尖峰优先正则化方法">2. 尖峰优先正则化方法</h2>
<h3 id="21-ctc-模型基础">2.1 CTC 模型基础</h3>
<p>CTC 模型能够直接建模了声学序列到文本序列的转换关系，而不需要注意力机制等结构。由于文本序列的长度远远小于声学特征序列（通常情况下声学特征序列以帧作为单位，相邻两帧之间间隔为 10ms，时长为 1s 的语音就可以被划分为100 帧），而在模型预测过程中，每帧特征都有一个预测标签。CTC 损失计算过程中引入了空格标记 φ 来作为填充标记符，以使得文本序列与声学序列的长度相等。</p>
<p>以下图(a)中所展示的 CTC 路径空间为例，其中横轴表示声学特征序列，纵轴表示目标文本序列。一条语音预测出文本序列“CAB”的概率可以被描述为后验概率$P(\text{CAB}∣{X})$， 为了方便计算损失，需要使用空格标记 φ 对文本标记填充，填充之后会出现与目标序列对应的多条路径（对应图3(a)中的实线与虚线，从图中左上角开始空格标记或者非空格标记开始，沿着线段转移至右下角空格或者非空格标记的路径均是一条可能的解码路径），所有路径的概率和等于后验概率 $P(\text{CAB}∣{X})$。为了避免路径穷举导致的计算爆炸问题，CTC 损失计算过程实际上使用了基于动态规划思想的前后向算法，来对所有可行的解码路径进行概率求和，并最终以负对数概率作为最终损失函数来进行优化。</p>
<h3 id="22-尖峰优先正则化方法描述">2.2 尖峰优先正则化方法描述</h3>
<p><img src="https://p1.meituan.net/travelcube/339ced816f0510bcb2190970dd91b4b6310938.png" alt="(a)CTC对齐路径空间(b)尖峰优先正则化计算过程"></p>
<p>由于 CTC 的输出概率中蕴含着海量的可行解码路径，为了降低输出延迟，我们对所有的解码路径进行了仔细的分析和观察，如上面(a)图所示，网格中包含与文本“CAB”对应的多条可能的路径，以橙色和蓝色实线连接的路径为例，显然两条路径的转移位置存在明显区别，蓝色路径分别在$t_2$，$t_4$和$t_6$位置预测出字符”C”，”A”，”B”；而橙色路径则在 $t_4$，$t_6$ 和 $t_9$ 位置才预测出对应的字符。因此蓝色路径相对橙色路径具有更低的出字延迟，其从时间轴上来看，蓝色路径相对橙色路径更加靠左。基于这个观察，我们可以得出结论：具有低延迟的路径在时间轴上的非空格标记概率尖峰的位置会更加靠前一些。因此，我们提出了一个猜想，可以通过将 CTC 输出的概率分布整体左移的方式来降低模型的出字延迟。</p>
<p>基于这个假设，本文提出了一个简单的正则化方法–尖峰优先正则化方法（Peak-First Regularization, PFR），来使得 CTC 模型的模型输出的概率分布实现整体左移以降低出字延迟。PFR 方法巧妙使用了知识蒸馏的方法，迫使 CTC 输出概率分布的每一帧概率都学习其邻近下一帧的信息。如上图(b)所示，利用逐帧的知识蒸馏函数，使得每一帧的概率分布 都学习其后一帧，随着迭代的进行，模型实现了整体分布的左移。其损失计算过程可以被表述为以下形式：</p>
<p><img src="https://p1.meituan.net/travelcube/378f950bf2872cf1eceea14fbd49e97c15161.png" alt=""></p>
<p>该损失函数仅作为正则项使用，整体损失函数可以被描述为：</p>
<p><img src="https://p0.meituan.net/travelcube/b4cf4cbf19119fbaa6dc31588cf023d88645.png" alt=""></p>
<p>其中 $\lambda$ 作为权重，用来平衡两个损失之间的关系，避免输出概率持续移动最终导致训练崩溃的问题。PFR 正则项在学习过程中实际上呈现损失数值上升的态势，当输出分布不再移动的时候，损失值也趋于平衡。</p>
<p>虽然模型在训练过程中仅学习后面一帧（约等于40 ms）的内容，但是随着训练结果的累积，可以获得远超 40ms 的延迟降低。这样设置有三方面的考虑：</p>
<ul>
<li>首先，模型在完全学习到下一帧内容后，整体分布已经左移了 40ms。再继续学习后一帧内容，可以实现延迟效果的累加。</li>
<li>其次，考虑到 CTC 的输出概率分布是稀疏的，如果学习后面第 N 帧的内容，有非常大的可能性第 N 帧是空格标记，起不到学习效果，甚至学不到时序移动的关系。</li>
<li>最后，仅学习后面一帧的内容会降低训练难度，这种情况下经过平滑后的相邻两帧之间的概率分布的相似程度比较高，比较容易直接学习。如果直接学习后面第 N 帧的内容，也容易使得模型初始情况下面临更加困难的学习环境。</li>
</ul>
<h3 id="23-梯度分析">2.3 梯度分析</h3>
<p>虽然本文通过间接的方法来实现降低延迟的效果，但是其仍然具有一定的解释性。可以对其梯度进行分析，假如 CTC 在第 t时刻预测出第 $k$ 个标记的概率是，则其梯度可以被描述为：</p>
<p>$$ \frac{\partial \mathcal{L}}{\partial p_{k}^t}=-\frac{G(t,k)+\lambda p_k^{t+1}}{p_k^t}$$</p>
<p>其中 CTC 损失部分梯度为：</p>
<p>$$\frac{\partial \mathcal{L}}{\partial p_{k}^t}=-\frac{G(t,k)}{p_k^t}$$</p>
<p>通过公式可以知道概率和它邻近下一帧的概率紧密相关，如果下一帧预测出同一个标记 $k$ 的概率很大（即下一帧是一个概率尖峰），则会促使当前帧梯度发生较大变化，进而实现概率分布左移，而如果下一帧的概率很小，则对当前帧梯度影响不大，不会产生概率分布位移。</p>
<h2 id="3-相关工作">3. 相关工作</h2>
<p>伴随着智能交互技术的发展，大家对于交互体验的要求越来越高，如何降低语音识别系统的出字延迟再次成为了研究热点，各种思路层出不穷。整体来看方法可以被归结为以下四类。</p>
<h3 id="31-强制对齐方法">3.1 强制对齐方法</h3>
<p>强制对齐（Force Alignment）方法依托外部模型提前生成强制对齐标注信息。这些信息中包含用户发音与标注文本之间的准确对齐关系，在 CTC 或者 Transducer 模型损失计算过程中对路径的延迟进行限制，对具有高延迟的路径施加惩罚，以此来实现降低延迟的目的[3,5]。这一方案将延迟作为约束引入到损失函数的计算过程中，需要修改损失函数以及梯度的计算环节。</p>
<h3 id="32-路径分解方法">3.2 路径分解方法</h3>
<p>路径分解方法以 FastEmit 方法为代表[4]，主要应用到 RNNT 模型上，其对 RNNT 损失计算过程中的每个节点进行了路径分解，在损失函数的计算过程中，对低延迟路径赋予更高的权重，进而达成了鼓励模型在空格标记和非空格标记中优先预测非空格标记来降低出字延迟的目的。</p>
<p>基于这种逻辑训练出来的模型具有较低的延迟。虽然该方法摆脱了对于强制对齐的依赖，可以使得模型在训练过程中自然而然得地学习到低延迟路径，但是这种方法仍然需要修改损失函数前向计算环节和修改梯度的计算公式，具体实现相对复杂。</p>
<h3 id="33-最小贝叶斯风险方法">3.3 最小贝叶斯风险方法</h3>
<p>最小贝叶斯风险方法（Bayes Risk CTC）方法将延迟作为贝叶斯风险值加入到损失函数的计算过程中[6]。为了避免大量的计算，使用了分而治之的策略来将路径分组，同组内指定相同的延迟风险值。本方法虽然能够获得延迟降低，但是需要修改损失函数和梯度的计算过程，甚至需要模型方面的改动，增加了延迟优化问题的复杂度。</p>
<h3 id="34-自对齐方法">3.4 自对齐方法</h3>
<p>自对齐方法（Self-Alignment）方法不需要修改损失函数的计算过程，而是从上一轮的模型的解码结果中选择低延迟路径，并将其作为正则项添加到本轮模型的优化过程中[7]，这种方法虽然简化了计算量，但是面临新的问题，这种在线解码的方法需要消耗大量的解码时间，当面临海量数据的时候，在线解码会严重训练的进度，延缓训练流程。</p>
<p>综上所述，本文提出的延迟优化策略最为简单，不需要复杂的损失与梯度计算，也不依赖外部强制对齐结果，且在小数据和大规模生产数据上同样有效。</p>
<h2 id="4-评价指标">4. 评价指标</h2>
<h3 id="41-字错误率">4.1 字错误率</h3>
<p>字错误率（Character Error Rate, CER）用来衡量标注文本与识别文本之间的编辑距离。字错误越低则语音识别结果越好。计算公式如下：</p>
<p><img src="https://p0.meituan.net/travelcube/578118932bebf045617f9ed136624e1167506.png" alt=""></p>
<h3 id="42-平均尖峰延迟平均出字延迟">4.2 平均尖峰延迟（平均出字延迟）</h3>
<p>平均尖峰延迟（Average Peak Latency, APL）是统计的每个解码正确的概率尖峰的首帧与通过强制对齐方法获得的每个文本标签人声范围的尾帧之间的时间差的平均值。这一指标反映了系统的平均延迟水平。平均尖峰延迟越低则意味着出字延迟越低，ASR 识别系统反馈识别结果的速度越快。</p>
<h3 id="43-pr50pr90">4.3 PR50/PR90</h3>
<p>由于真实的 CTC 预测的出字延迟分布具有长尾分布的特点，所以引入了 尖峰延迟的 50 分位数和 90 分位数来衡量延迟分布的特点。其计算方法是根据每句话的平均出字延迟进行从小到大进行排序，以整个分部中第 50% 和 90% 条句子的平均出字延迟作为指标。PR50/PR90 越低表示整个长尾分布的尾巴越短，长尾分布中尾巴部分的数据延迟越低，比例越小。</p>
<h2 id="5-实验与分析">5. 实验与分析</h2>
<h3 id="51-实验与模型搭建">5.1 实验与模型搭建</h3>
<p>本文基于开源中文语音识别数据集 AISHELL-1 进行实验，并采用了流式和非流式两种模型进行比较验证。两个模型均为 Transformer 模型结构，包含两层 2D 卷积构建的前端特征处理模块，以及 12 层 Transformer 编码层构建的编码模块以及一个输出线性映射层。其中流式模型依赖 510ms 的声学下文。</p>
<h3 id="52-出字延迟比较">5.2 出字延迟比较</h3>
<p><img src="https://p0.meituan.net/travelcube/66f874076f5694ea260f02906bdb2f6a496613.png" alt="实验结果"></p>
<p>上图中分别展示了流式识别模型与非流式识别模型在开源测试集上的字准确率和延迟结果。从实验结果很容易发现，无论是非流式模型还是流式模型，采用本文提出的尖峰优先正则化方法均能够降低各种延迟指标，并且延迟的变化值与权重的设置关系密切。通过调节权重设置可以显著改变延迟的大小，权重设置越大，延迟越低。在字错误率（CER）不降低的条件下，非流式模型和流式模型在测试集中分别可以获得 149 毫秒和 101 毫秒的延迟降低，在 CER波动的可接受范围内，延迟甚至可以降低200 毫秒以上。</p>
<p>当权重设置比较小的时候，模型甚至能够同时获得CER 和平均延迟的降低，我们猜想造成这种现象的原因是正则化使得模型在学习邻近帧的时候同时学到了更长的声学下文信息。随着权重的变大，模型的识别错误率可能出现上升，此时权重的设置已经破坏了两个损失之间的平衡关系，模型在训练过程中会更激进地倾向于选择低延迟的路径，这种情况下会损失更多的声学下文信息，造成识别结果的衰退。</p>
<h3 id="53-可视化分析">5.3 可视化分析</h3>
<p><img src="https://p1.meituan.net/travelcube/0fca65adf04b981323bd9f8a053a9581671819.png" alt="实验结果"></p>
<p>本文随后通过可视化的方式对系统的延迟变化进行分析。上图中左侧三幅图表示非流式模型的输出概率分布，右侧三幅图则表示流式模型的输出概率分布。图上方的色块与概率尖峰一一对应，便是每个标记的发声范围，而下面的概率尖峰则表示 CTC 预测到对应标记的位置与概率。</p>
<p>很容易发现图中非流式模型中原本每个尖峰的位置就处于其对应的发声范围中，引入尖峰优先策略后尖峰的位置甚至能够提前其发声范围。而流式模型的概率尖峰也往往滞后于其发声范围，而引入尖峰优先策略后同样可以获得较大的延迟降低效果。通过图中不同权重的参数设置也可以发现，使用较大的权重能够更大程度地降低系统延迟。</p>
<h2 id="6-总结与展望">6. 总结与展望</h2>
<p>本文通过对 CTC 的输出概率分布进行分析，将 CTC 的出字延迟问题转化为一个知识蒸馏过程。通过知识蒸馏方法将 CTC 的输出概率分布沿着时间轴左移，从而有效地降低 CTC 模型的出字延迟。本文提出的方法简单有效，不需要强制对齐标注信息，也不需要复杂的损失和梯度计算方法。此外该方法也具有一定的扩展空间，或许可以扩展到 Transducer 等语音识别模型上。</p>
<h2 id="7-本文作者">7. 本文作者</h2>
<p>正坤、鸿雨、李敏、飞飞、丁科、广鲁等，均来自美团平台/语音交互部。</p>
<h2 id="8-参考文献">8. 参考文献</h2>
<ul>
<li>[1] Alex Graves, Santiago FernÅLandez, Faustino Gomez, and JÅNurgen Schmidhuber, “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.</li>
<li>[2] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” in International conference on machine learning, 2016, pp. 173–182.</li>
<li>[3] Andrew Senior, Has.im Sak, FÅLelix de Chaumont Quitry, Tara Sainath, and Kanishka Rao, “Acoustic modelling with cd-ctcsmbr lstm rnns,” in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 604– 609.</li>
<li>[4] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo-yiin Chang, Tara N Sainath, Yanzhang He, Arun Narayanan,Wei Han, Anmol Gulati, Yonghui Wu, et al., “Fastemit: Low-latency streaming asr with sequence-level emission regularization,” in ICASSP 2021- 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6004–6008.</li>
<li>[5] Yusuke Shinohara and Shinji Watanabe, “Minimum latency training of sequence transducers for streaming end-to-end speech recognition,” in Proc. Interspeech 2022, 2022, pp.2098–2102.</li>
<li>[6] Jinchuan Tian, Brian Yan, Jianwei Yu, Chao Weng, Dong Yu, and Shinji Watanabe, “Bayes risk ctc: Controllable ctc alignment in sequence-to-sequence tasks,” arXiv preprint arXiv:2210.07499, 2022.</li>
<li>[7] Jaeyoung Kim, Han Lu, Anshuman Tripathi, Qian Zhang, and Hasim Sak, “Reducing streaming asr model delay with self alignment,” arXiv preprint arXiv:2105.05005, 2021.</li>
<li>[8] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al., “Espnet: End-to-end speech processing toolkit,” arXiv preprint arXiv:1804.00015, 2018.</li>
</ul>

		</div>

		<div class="post-tags">
			
				
					<nav class="nav tags">
							<ul class="flat">
								
								<li><a href="/tags/%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F">美团技术团队</a></li>
								
								<li><a href="/tags/%E7%BE%8E%E5%9B%A2%E5%B9%B3%E5%8F%B0">美团平台</a></li>
								
								<li><a href="/tags/%E7%AE%97%E6%B3%95">算法</a></li>
								
								<li><a href="/tags/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB">语音识别</a></li>
								
								<li><a href="/tags/%E8%AF%AD%E9%9F%B3%E4%BA%A4%E4%BA%92">语音交互</a></li>
								
							</ul>
					</nav>
				
			
		</div>
		<div id="disqus_thread"></div>
<script type="text/javascript">
	(function () {
		
		
		if (window.location.hostname == "localhost")
			return;

		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		var disqus_shortname = 'wfsui';
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
		Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> © Copyright notice |  <a href="">Wfsui theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div><script>feather.replace()</script>
</body>
</html>
