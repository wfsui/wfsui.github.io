<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>咱们从头到尾讲一次 Flink 网络流控和反压剖析 - 大峰哥</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="咱们从头到尾讲一次 Flink 网络流控和反压剖析" />
<meta property="og:description" content="作者：张俊
整理：张友亮（Apache Flink 社区志愿者）
本文共 4745字，预计阅读时间 15min。
本文根据 Apache Flink 系列直播整理而成，由 Apache Flink Contributor、OPPO 大数据平台研发负责人张俊老师分享。主要内容如下：
网络流控的概念与背景 TCP的流控机制 Flink TCP-based 反压机制（before V1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wfsui.github.io/posts/%E5%92%B1%E4%BB%AC%E4%BB%8E%E5%A4%B4%E5%88%B0%E5%B0%BE%E8%AE%B2%E4%B8%80%E6%AC%A1-flink-%E7%BD%91%E7%BB%9C%E6%B5%81%E6%8E%A7%E5%92%8C%E5%8F%8D%E5%8E%8B%E5%89%96%E6%9E%90/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-27T02:40:11+00:00" />
<meta property="article:modified_time" content="2024-03-27T02:40:11+00:00" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="咱们从头到尾讲一次 Flink 网络流控和反压剖析"/>
<meta name="twitter:description" content="作者：张俊
整理：张友亮（Apache Flink 社区志愿者）
本文共 4745字，预计阅读时间 15min。
本文根据 Apache Flink 系列直播整理而成，由 Apache Flink Contributor、OPPO 大数据平台研发负责人张俊老师分享。主要内容如下：
网络流控的概念与背景 TCP的流控机制 Flink TCP-based 反压机制（before V1."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://wfsui.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://wfsui.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://wfsui.github.io/css/dark.css" media="(prefers-color-scheme: dark)" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="https://wfsui.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<base href="https://wfsui.github.io/">
	<h1 class="site-title"><a href="https://wfsui.github.io/">大峰哥</a></h1>
	<div class="site-description"><h2>记录日常生活哦</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/wfsui" title="Github"><i data-feather="github"></i></a><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">首页</a>
			</li>
			
			<li>
				<a href="/posts">文章</a>
			</li>
			
			<li>
				<a href="/about">关于</a>
			</li>
			
			<li>
				<a href="/tags">标签</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">咱们从头到尾讲一次 Flink 网络流控和反压剖析</h1>
			<div class="meta">Posted at &mdash; Mar 27, 2024</div>
		</div>

		<div class="markdown">
			<p>作者：张俊</p>
<p>整理：张友亮（Apache Flink 社区志愿者）</p>
<blockquote>
<p>本文共 4745字，预计阅读时间 15min。</p>
</blockquote>
<p>本文根据 Apache Flink 系列直播整理而成，由 Apache Flink Contributor、OPPO 大数据平台研发负责人张俊老师分享。主要内容如下：</p>
<ul>
<li>网络流控的概念与背景</li>
<li>TCP的流控机制</li>
<li>Flink TCP-based 反压机制（before V1.5）</li>
<li>Flink Credit-based 反压机制 （since V1.5）</li>
<li>总结与思考</li>
</ul>
<h2 id="网络流控的概念与背景">网络流控的概念与背景</h2>
<h3 id="为什么需要网络流控">为什么需要网络流控</h3>
<p><img alt="1_04" src="https://yqfile.alicdn.com/f55df72c2c254caf9a92fbe3890df48583dfae4d.png" title="1_04"></p>
<p>首先我们可以看下这张最精简的网络流控的图，Producer 的吞吐率是 2MB/s，Consumer 是 1MB/s，这个时候我们就会发现在网络通信的时候我们的 Producer 的速度是比 Consumer 要快的，有 1MB/s 的这样的速度差，假定我们两端都有一个 Buffer，Producer 端有一个发送用的 Send Buffer，Consumer 端有一个接收用的 Receive Buffer，在网络端的吞吐率是 2MB/s，过了 5s 后我们的 Receive Buffer 可能就撑不住了，这时候会面临两种情况：</p>
<ul>
<li>1.如果 Receive Buffer 是有界的，这时候新到达的数据就只能被丢弃掉了。</li>
<li>2.如果 Receive Buffer 是无界的，Receive Buffer 会持续的扩张，最终会导致 Consumer 的内存耗尽。</li>
</ul>
<h3 id="网络流控的实现静态限速">网络流控的实现：静态限速</h3>
<p><img alt="2_05" src="https://yqfile.alicdn.com/52aca194fd13dc63d0ec1811e3535ab470a91e49.png" title="2_05"></p>
<p>为了解决这个问题，我们就需要网络流控来解决上下游速度差的问题，传统的做法可以在 Producer 端实现一个类似 Rate Limiter 这样的静态限流，Producer 的发送速率是 2MB/s，但是经过限流这一层后，往 Send Buffer 去传数据的时候就会降到 1MB/s 了，这样的话 Producer 端的发送速率跟 Consumer 端的处理速率就可以匹配起来了，就不会导致上述问题。但是这个解决方案有两点限制：</p>
<ul>
<li>1、事先无法预估 Consumer 到底能承受多大的速率</li>
<li>2、 Consumer 的承受能力通常会动态地波动</li>
</ul>
<h3 id="网络流控的实现动态反馈自动反压">网络流控的实现：动态反馈/自动反压</h3>
<p><img alt="3_06" src="https://yqfile.alicdn.com/5d3f0b587792ae53f5842bcf0e0253a7e52f0e4f.png" title="3_06"></p>
<p>针对静态限速的问题我们就演进到了动态反馈（自动反压）的机制，我们需要 Consumer 能够及时的给 Producer 做一个 feedback，即告知 Producer 能够承受的速率是多少。动态反馈分为两种：</p>
<ul>
<li>1、负反馈：接受速率小于发送速率时发生，告知 Producer 降低发送速率</li>
<li>2、正反馈：发送速率小于接收速率时发生，告知 Producer 可以把发送速率提上来</li>
</ul>
<p>让我们来看几个经典案例</p>
<h3 id="案例一storm-反压实现">案例一：Storm 反压实现</h3>
<p><img alt="4_07" src="https://yqfile.alicdn.com/7dad1c84b9210e1d3c99591c633b586438de9227.png" title="4_07"></p>
<p>上图就是 Storm 里实现的反压机制，可以看到 Storm 在每一个 Bolt 都会有一个监测反压的线程（Backpressure Thread），这个线程一但检测到 Bolt 里的接收队列（recv queue）出现了严重阻塞就会把这个情况写到 ZooKeeper 里，ZooKeeper 会一直被 Spout 监听，监听到有反压的情况就会停止发送，通过这样的方式匹配上下游的发送接收速率。</p>
<h3 id="案例二spark-streaming-反压实现">案例二：Spark Streaming 反压实现</h3>
<p><img alt="5_08" src="https://yqfile.alicdn.com/e7b4e415ccb4311ab95f455ce7f1c1b432206da1.png" title="5_08"></p>
<p>Spark Streaming 里也有做类似这样的 feedback 机制，上图 Fecher 会实时的从 Buffer、Processing 这样的节点收集一些指标然后通过 Controller 把速度接收的情况再反馈到 Receiver，实现速率的匹配。</p>
<h3 id="疑问为什么-flinkbefore-v15里没有用类似的方式实现-feedback-机制">疑问：为什么 Flink（before V1.5）里没有用类似的方式实现 feedback 机制？</h3>
<p>首先在解决这个疑问之前我们需要先了解一下 Flink 的网络传输是一个什么样的架构。</p>
<p><img alt="6_10" src="https://yqfile.alicdn.com/58006efbd154eebbf8110f7787b4d94bec451326.png" title="6_10"></p>
<p>这张图就体现了 Flink 在做网络传输的时候基本的数据的流向，发送端在发送网络数据前要经历自己内部的一个流程，会有一个自己的 Network Buffer，在底层用 Netty 去做通信，Netty 这一层又有属于自己的 ChannelOutbound Buffer，因为最终是要通过 Socket 做网络请求的发送，所以在 Socket 也有自己的 Send Buffer，同样在接收端也有对应的三级 Buffer。学过计算机网络的时候我们应该了解到，TCP 是自带流量控制的。实际上 Flink （before V1.5）就是通过 TCP 的流控机制来实现 feedback 的。</p>
<h2 id="tcp-流控机制">TCP 流控机制</h2>
<p>根据下图我们来简单的回顾一下 TCP 包的格式结构。首先，他有 Sequence number 这样一个机制给每个数据包做一个编号，还有 ACK number 这样一个机制来确保 TCP 的数据传输是可靠的，除此之外还有一个很重要的部分就是 Window Size，接收端在回复消息的时候会通过 Window Size 告诉发送端还可以发送多少数据。</p>
<p><img alt="7_13" src="https://yqfile.alicdn.com/0b4bc72d3d02bb6f1c1b24ec08e0259602651fb3.png" title="7_13"></p>
<p>接下来我们来简单看一下这个过程。</p>
<h3 id="tcp-流控滑动窗口">TCP 流控：滑动窗口</h3>
<p><img alt="8_14" src="https://yqfile.alicdn.com/7efa6c17cab155ed971fe0712d55ea5efb2465f5.png" title="8_14"></p>
<p>TCP 的流控就是基于滑动窗口的机制，现在我们有一个 Socket 的发送端和一个 Socket 的接收端，目前我们的发送端的速率是我们接收端的 3 倍，这样会发生什么样的一个情况呢？假定初始的时候我们发送的 window 大小是 3，然后我们接收端的 window 大小是固定的，就是接收端的 Buffer 大小为 5。</p>
<p><img alt="9_15" src="https://yqfile.alicdn.com/f7e349d298e1e2c98a7f37f60023bd56922c6b08.png" title="9_15"></p>
<p>首先，发送端会一次性发 3 个 packets，将 1，2，3 发送给接收端，接收端接收到后会将这 3 个 packets 放到 Buffer 里去。</p>
<p><img alt="10_16" src="https://yqfile.alicdn.com/df81be90f206d36610ac86784e3d069ba8443f59.png" title="10_16"></p>
<p>接收端一次消费 1 个 packet，这时候 1 就已经被消费了，然后我们看到接收端的滑动窗口会往前滑动一格，这时候 2，3 还在 Buffer 当中 而 4，5，6 是空出来的，所以接收端会给发送端发送 ACK = 4 ，代表发送端可以从 4 开始发送，同时会将 window 设置为 3 （Buffer 的大小 5 减去已经存下的 2 和 3），发送端接收到回应后也会将他的滑动窗口向前移动到 4，5，6。</p>
<p><img alt="11_17" src="https://yqfile.alicdn.com/47bdef8db4a32262325a45ad0c46a20fec126887.png" title="11_17"></p>
<p>这时候发送端将 4，5，6 发送，接收端也能成功的接收到 Buffer 中去。</p>
<p><img alt="12_18" src="https://yqfile.alicdn.com/58bf284147fecc3415ab75b935b927c093cb2b2b.png" title="12_18"></p>
<p>到这一阶段后，接收端就消费到 2 了，同样他的窗口也会向前滑动一个，这时候他的 Buffer 就只剩一个了，于是向发送端发送 ACK = 7、window = 1。发送端收到之后滑动窗口也向前移，但是这个时候就不能移动 3 格了，虽然发送端的速度允许发 3 个 packets 但是 window 传值已经告知只能接收一个，所以他的滑动窗口就只能往前移一格到 7 ，这样就达到了限流的效果，发送端的发送速度从 3 降到 1。</p>
<p><img alt="13_19" src="https://yqfile.alicdn.com/c69da59e1f5ba7f1c3371018de0c42ad4456912d.png" title="13_19"></p>
<p><img alt="14_20" src="https://yqfile.alicdn.com/a9a28aba06629691eaa7f577e3b2b5f9bc3b2eb0.png" title="14_20"></p>
<p>我们再看一下这种情况，这时候发送端将 7 发送后，接收端接收到，但是由于接收端的消费出现问题，一直没有从 Buffer 中去取，这时候接收端向发送端发送 ACK = 8、window = 0 ，由于这个时候 window = 0，发送端是不能发送任何数据，也就会使发送端的发送速度降为 0。这个时候发送端不发送任何数据了，接收端也不进行任何的反馈了，那么如何知道消费端又开始消费了呢？</p>
<p><img alt="15_21" src="https://yqfile.alicdn.com/101ebbd06f370649cc395438a96a98a8e54e137d.png" title="15_21"></p>
<p><img alt="16_22" src="https://yqfile.alicdn.com/e636403b67a99c0b153fe0c2fe527904f271408b.png" title="16_22"></p>
<p><img alt="17_23" src="https://yqfile.alicdn.com/50a7f6d0b4a9ff37262116ea716d666d8be96315.png" title="17_23"></p>
<p>TCP 当中有一个 ZeroWindowProbe 的机制，发送端会定期的发送 1 个字节的探测消息，这时候接收端就会把 window 的大小进行反馈。当接收端的消费恢复了之后，接收到探测消息就可以将 window 反馈给发送端端了从而恢复整个流程。TCP 就是通过这样一个滑动窗口的机制实现 feedback。</p>
<h2 id="flink-tcp-based-反压机制before-v15">Flink TCP-based 反压机制（before V1.5）</h2>
<h3 id="示例windowwordcount">示例：WindowWordCount</h3>
<p><img alt="18_25" src="https://yqfile.alicdn.com/140ac4a6865e579875d4cca8aa9c5002d3eae2df.png" title="18_25"></p>
<p>大体的逻辑就是从 Socket 里去接收数据，每 5s 去进行一次 WordCount，将这个代码提交后就进入到了编译阶段。</p>
<h3 id="编译阶段生成-jobgraph">编译阶段：生成 JobGraph</h3>
<p><img alt="19_26" src="https://yqfile.alicdn.com/4d05a32a46d7db50325b8ce00e0be672ec1ba9a7.png" title="19_26"></p>
<p>这时候还没有向集群去提交任务，在 Client 端会将 StreamGraph 生成 JobGraph，JobGraph 就是做为向集群提交的最基本的单元。在生成 JobGrap 的时候会做一些优化，将一些没有 Shuffle 机制的节点进行合并。有了 JobGraph 后就会向集群进行提交，进入运行阶段。</p>
<h3 id="运行阶段调度-executiongraph">运行阶段：调度 ExecutionGraph</h3>
<p><img alt="20_27" src="https://yqfile.alicdn.com/e916c83ad03e7e29ed5810974499adcce540524b.png" title="20_27"></p>
<p>JobGraph 提交到集群后会生成 ExecutionGraph ，这时候就已经具备基本的执行任务的雏形了，把每个任务拆解成了不同的 SubTask，上图 ExecutionGraph 中的 Intermediate Result Partition 就是用于发送数据的模块，最终会将 ExecutionGraph 交给 JobManager 的调度器，将整个 ExecutionGraph 调度起来。然后我们概念化这样一张物理执行图，可以看到每个 Task 在接收数据时都会通过这样一个 InputGate 可以认为是负责接收数据的，再往前有这样一个 ResultPartition 负责发送数据，在 ResultPartition 又会去做分区跟下游的 Task 保持一致，就形成了 ResultSubPartition 和 InputChannel 的对应关系。这就是从逻辑层上来看的网络传输的通道，基于这么一个概念我们可以将反压的问题进行拆解。</p>
<h3 id="问题拆解反压传播两个阶段">问题拆解：反压传播两个阶段</h3>
<p><img alt="21_28" src="https://yqfile.alicdn.com/4a4a9e76dd9843819387b0e5ed25820029efde0e.png" title="21_28"></p>
<p>反压的传播实际上是分为两个阶段的，对应着上面的执行图，我们一共涉及 3 个 TaskManager，在每个 TaskManager 里面都有相应的 Task 在执行，还有负责接收数据的 InputGate，发送数据的 ResultPartition，这就是一个最基本的数据传输的通道。在这时候假设最下游的 Task （Sink）出现了问题，处理速度降了下来这时候是如何将这个压力反向传播回去呢？这时候就分为两种情况：</p>
<ul>
<li>跨 TaskManager ，反压如何从 InputGate 传播到 ResultPartition</li>
<li>TaskManager 内，反压如何从 ResultPartition 传播到 InputGate</li>
</ul>
<h3 id="跨-taskmanager-数据传输">跨 TaskManager 数据传输</h3>
<p><img alt="22_29" src="https://yqfile.alicdn.com/67ca07e2eaecd5190982c5ecc7e127960de0d83e.png" title="22_29"></p>
<p>前面提到，发送数据需要 ResultPartition，在每个 ResultPartition 里面会有分区 ResultSubPartition，中间还会有一些关于内存管理的 Buffer。</p>
<p>对于一个 TaskManager 来说会有一个统一的 Network BufferPool 被所有的 Task 共享，在初始化时会从 Off-heap Memory 中申请内存，申请到内存的后续内存管理就是同步 Network BufferPool 来进行的，不需要依赖 JVM GC 的机制去释放。有了 Network BufferPool 之后可以为每一个 ResultSubPartition 创建 Local BufferPool 。</p>
<p>如上图左边的 TaskManager 的 Record Writer 写了 &lt;1，2&gt; 这个两个数据进来，因为 ResultSubPartition 初始化的时候为空，没有 Buffer 用来接收，就会向 Local BufferPool 申请内存，这时 Local BufferPool 也没有足够的内存于是将请求转到 Network BufferPool，最终将申请到的 Buffer 按原链路返还给 ResultSubPartition，&lt;1，2&gt; 这个两个数据就可以被写入了。之后会将 ResultSubPartition 的 Buffer 拷贝到 Netty 的 Buffer 当中最终拷贝到 Socket 的 Buffer 将消息发送出去。然后接收端按照类似的机制去处理将消息消费掉。</p>
<p>接下来我们来模拟上下游处理速度不匹配的场景，发送端的速率为 2，接收端的速率为 1，看一下反压的过程是怎样的。</p>
<h3 id="跨-taskmanager-反压过程">跨 TaskManager 反压过程</h3>
<p><img alt="23_30" src="https://yqfile.alicdn.com/72a01465aa1d9a5bf269d3c0efb69daeeda14f16.png" title="23_30"></p>
<p>因为速度不匹配就会导致一段时间后 InputChannel 的 Buffer 被用尽，于是他会向 Local BufferPool 申请新的 Buffer ，这时候可以看到 Local BufferPool 中的一个 Buffer 就会被标记为 Used。</p>
<p><img alt="24_31" src="https://yqfile.alicdn.com/c579c55399d69082e7261a8ee6b8f36d570d8159.png" title="24_31"></p>
<p>发送端还在持续以不匹配的速度发送数据，然后就会导致 InputChannel 向 Local BufferPool 申请 Buffer 的时候发现没有可用的 Buffer 了，这时候就只能向 Network BufferPool 去申请，当然每个 Local BufferPool 都有最大的可用的 Buffer，防止一个 Local BufferPool 把 Network BufferPool 耗尽。这时候看到 Network BufferPool 还是有可用的 Buffer 可以向其申请。</p>
<p><img alt="25_32" src="https://yqfile.alicdn.com/58fcb2180a4308d4d19c63726917501ea0dbe691.png" title="25_32"></p>
<p>一段时间后，发现 Network BufferPool 没有可用的 Buffer，或是 Local BufferPool 的最大可用 Buffer 到了上限无法向 Network BufferPool 申请，没有办法去读取新的数据，这时 Netty AutoRead 就会被禁掉，Netty 就不会从 Socket 的 Buffer 中读取数据了。</p>
<p><img alt="26_33" src="https://yqfile.alicdn.com/4a01bb1aabecd29f39d9199024590c42cb5ae3c1.png" title="26_33"></p>
<p>显然，再过不久 Socket 的 Buffer 也被用尽，这时就会将 Window = 0 发送给发送端（前文提到的 TCP 滑动窗口的机制）。这时发送端的 Socket 就会停止发送。</p>
<p><img alt="27_34" src="https://yqfile.alicdn.com/151369e71d92a1c82baa27b4396043c88fa90541.png" title="27_34"></p>
<p>很快发送端的 Socket 的 Buffer 也被用尽，Netty 检测到 Socket 无法写了之后就会停止向 Socket 写数据。</p>
<p><img alt="28_35" src="https://yqfile.alicdn.com/6c57a0adb9d3ab71f8f4b7c5a2324f0e370d2bfe.png" title="28_35"></p>
<p>Netty 停止写了之后，所有的数据就会阻塞在 Netty 的 Buffer 当中了，但是 Netty 的 Buffer 是无界的，可以通过 Netty 的水位机制中的 high watermark 控制他的上界。当超过了 high watermark，Netty 就会将其 channel 置为不可写，ResultSubPartition 在写之前都会检测 Netty 是否可写，发现不可写就会停止向 Netty 写数据。</p>
<p><img alt="29_36" src="https://yqfile.alicdn.com/0de9eb334c31c720c472e5e24cc206c00d9c72e0.png" title="29_36"></p>
<p>这时候所有的压力都来到了 ResultSubPartition，和接收端一样他会不断的向 Local BufferPool 和 Network BufferPool 申请内存。</p>
<p><img alt="30_38" src="https://yqfile.alicdn.com/898b316178ec08b7c5b2736294337272046040f8.png" title="30_38"></p>
<p>Local BufferPool 和 Network BufferPool 都用尽后整个 Operator 就会停止写数据，达到跨 TaskManager 的反压。</p>
<h3 id="taskmanager-内反压过程">TaskManager 内反压过程</h3>
<p>了解了跨 TaskManager 反压过程后再来看 TaskManager 内反压过程就更好理解了，下游的 TaskManager 反压导致本 TaskManager 的 ResultSubPartition 无法继续写入数据，于是 Record Writer 的写也被阻塞住了，因为 Operator 需要有输入才能有计算后的输出，输入跟输出都是在同一线程执行， Record Writer 阻塞了，Record Reader 也停止从 InputChannel 读数据，这时上游的 TaskManager 还在不断地发送数据，最终将这个 TaskManager 的 Buffer 耗尽。具体流程可以参考下图，这就是 TaskManager 内的反压过程。</p>
<p><img alt="31_39" src="https://yqfile.alicdn.com/f6669644615c925f089e2fb6d3d20fabb2439a89.png" title="31_39"></p>
<p><img alt="32_40" src="https://yqfile.alicdn.com/4463f83549c16ffa47902f91c44b1e47200061c4.png" title="32_40"></p>
<p><img alt="33_41" src="https://yqfile.alicdn.com/a4816d7bdcfed08e89430ccd919943f1b8a341ea.png" title="33_41"></p>
<p><img alt="34_42" src="https://yqfile.alicdn.com/0bc094c7fb61f41c3b2f66f02029c2d527ec89bd.png" title="34_42"></p>
<h2 id="flink-credit-based-反压机制since-v15">Flink Credit-based 反压机制（since V1.5）</h2>
<h3 id="tcp-based-反压的弊端">TCP-based 反压的弊端</h3>
<p><img alt="35_44" src="https://yqfile.alicdn.com/570debeba11e71ffac5f5b78f04e2bc88179e852.png" title="35_44"></p>
<p>在介绍 Credit-based 反压机制之前，先分析下 TCP 反压有哪些弊端。</p>
<ul>
<li>在一个 TaskManager 中可能要执行多个 Task，如果多个 Task 的数据最终都要传输到下游的同一个 TaskManager 就会复用同一个 Socket 进行传输，这个时候如果单个 Task 产生反压，就会导致复用的 Socket 阻塞，其余的 Task 也无法使用传输，checkpoint barrier 也无法发出导致下游执行 checkpoint 的延迟增大。</li>
<li>依赖最底层的 TCP 去做流控，会导致反压传播路径太长，导致生效的延迟比较大。</li>
</ul>
<h3 id="引入-credit-based-反压">引入 Credit-based 反压</h3>
<p>这个机制简单的理解起来就是在 Flink 层面实现类似 TCP 流控的反压机制来解决上述的弊端，Credit 可以类比为 TCP 的 Window 机制。</p>
<h3 id="credit-based-反压过程">Credit-based 反压过程</h3>
<p><img alt="36_46" src="https://yqfile.alicdn.com/c61481b75ec8d5ae62af8ce4ea45bd9d171e9dce.png" title="36_46"></p>
<p>如图所示在 Flink 层面实现反压机制，就是每一次 ResultSubPartition 向 InputChannel 发送消息的时候都会发送一个 backlog size 告诉下游准备发送多少消息，下游就会去计算有多少的 Buffer 去接收消息，算完之后如果有充足的 Buffer 就会返还给上游一个 Credit 告知他可以发送消息（图上两个 ResultSubPartition 和 InputChannel 之间是虚线是因为最终还是要通过 Netty 和 Socket 去通信），下面我们看一个具体示例。</p>
<p><img alt="37_47" src="https://yqfile.alicdn.com/a9a211ea22afc8a07d55fc3feb168b69c37f3643.png" title="37_47"></p>
<p>假设我们上下游的速度不匹配，上游发送速率为 2，下游接收速率为 1，可以看到图上在 ResultSubPartition 中累积了两条消息，10 和 11， backlog 就为 2，这时就会将发送的数据 &lt;8,9&gt; 和 backlog = 2 一同发送给下游。下游收到了之后就会去计算是否有 2 个 Buffer 去接收，可以看到 InputChannel 中已经不足了这时就会从 Local BufferPool 和 Network BufferPool 申请，好在这个时候 Buffer 还是可以申请到的。</p>
<p><img alt="38_48" src="https://yqfile.alicdn.com/be73146308876ade5e8b24e97413e0a7a71d3c8f.png" title="38_48"></p>
<p>过了一段时间后由于上游的发送速率要大于下游的接受速率，下游的 TaskManager 的 Buffer 已经到达了申请上限，这时候下游就会向上游返回 Credit = 0，ResultSubPartition 接收到之后就不会向 Netty 去传输数据，上游 TaskManager 的 Buffer 也很快耗尽，达到反压的效果，这样在 ResultSubPartition 层就能感知到反压，不用通过 Socket 和 Netty 一层层地向上反馈，降低了反压生效的延迟。同时也不会将 Socket 去阻塞，解决了由于一个 Task 反压导致 TaskManager 和 TaskManager 之间的 Socket 阻塞的问题。</p>
<h2 id="总结与思考">总结与思考</h2>
<h3 id="总结">总结</h3>
<ul>
<li>网络流控是为了在上下游速度不匹配的情况下，防止下游出现过载</li>
<li>网络流控有静态限速和动态反压两种手段</li>
<li>Flink 1.5 之前是基于 TCP 流控 + bounded buffer 实现反压</li>
<li>Flink 1.5 之后实现了自己托管的 credit - based 流控机制，在应用层模拟 TCP 的流控机制</li>
</ul>
<h3 id="思考">思考</h3>
<p>有了动态反压，静态限速是不是完全没有作用了？</p>
<p><img alt="39_52" src="https://yqfile.alicdn.com/fe6df2957ec9255f8b2958d9a78feb4719b4915a.png" title="39_52"></p>
<p>实际上动态反压不是万能的，我们流计算的结果最终是要输出到一个外部的存储（Storage），外部数据存储到 Sink 端的反压是不一定会触发的，这要取决于外部存储的实现，像 Kafka 这样是实现了限流限速的消息中间件可以通过协议将反压反馈给 Sink 端，但是像 ES 无法将反压进行传播反馈给 Sink 端，这种情况下为了防止外部存储在大的数据量下被打爆，我们就可以通过静态限速的方式在 Source 端去做限流。所以说动态反压并不能完全替代静态限速的，需要根据合适的场景去选择处理方案。</p>
<hr>
<p><strong>▼ Apache Flink 社区推荐 ▼</strong></p>
<p>Apache Flink 及大数据领域顶级盛会 Flink Forward Asia 2019 大会议程重磅发布，参与<a href="https://survey.aliyun.com/apps/zhiliao/yvZ60Qj6">问卷调研</a>就有机会免费获取门票！</p>
<p><a href="https://developer.aliyun.com/special/ffa2019">https://developer.aliyun.com/special/ffa2019</a></p>

		</div>

		<div class="post-tags">
			
				
					<nav class="nav tags">
							<ul class="flat">
								
								<li><a href="/tags/%E9%9D%A2%E8%AF%95">面试</a></li>
								
							</ul>
					</nav>
				
			
		</div>
		<div id="disqus_thread"></div>
<script type="text/javascript">
	(function () {
		
		
		if (window.location.hostname == "localhost")
			return;

		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		var disqus_shortname = 'wfsui';
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
		Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> © Copyright notice |  <a href="">Wfsui theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div><script>feather.replace()</script>
</body>
</html>
