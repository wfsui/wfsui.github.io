<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>如何提供一个可信的AB测试解决方案 - 大峰哥</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="如何提供一个可信的AB测试解决方案" />
<meta property="og:description" content="1 背景 虽然AB测试（AB实验）的统计基础已经有一个世纪的历史了，但大规模地构建一个正确可靠的A/B测试平台仍然是一个巨大的挑战：不仅要在实验设计环节应对溢出效应和小样本的双重挑战，平衡好实验偏差与方差以确定合适的实验单元、分组方法和分析方法，给出合理的实验设计，而且要在分析环节应对方差计算、P值计算、多重比较、混淆因素、假阴性（实际策略有效果，但是检测显示无效果）等多种统计陷阱。因此，要获得高质量的结果需要对实验和统计有专家级的理解，这无疑增加了实验门槛，难以达成任何人进行实验都可得出可信结论的目标。
本文将从实验方法和平台建设的两个视角，分别介绍如何正确地使用统计方法避免统计陷阱，以及输出什么样的平台能力，从而确保任何人使用该平台时都可得出可信结论。同时，我们也积累了如何进行更好的实验，以及如何利用实验来做出更好的决策，希望能给从事相关工作的同学有所帮助，也真诚地希望欢迎大家给出反馈或者建议，不断优化我们的工作。
2 走进AB测试 哪个线上选项会更好？我们经常需要做出这样的选择。当我们想要在两个策略之间做出决定时，理想的方案是面向同一拨用户，在两个平行时空，平行时空1体验原策略A，平行时空2体验新策略B，然后根据观测到的事实进行比较，以决定哪个策略胜出。然而在现实世界中，不存在两个平行时空，针对同一用户，我们只能观察到其接受策略A或策略B的一种效果，即反事实结果是观测不到的。
因此，在现实世界中，我们通常采用实验的方法做出决策。它将用户分配到不同的组，同一组内的用户在实验期间使用相同的策略，不同组的用户使用不同的策略。同时，日志系统根据实验系统为用户打标记，用于记录用户的行为，然后根据带有标记的日志计算度量差异，并进行统计分析以排除由于噪声导致的任何差异。实验者通过这些指标去理解和分析不同的策略对用户起了什么样的作用，是否符合实验预先假设。
2.1 AB测试概述 实证中由于不可能同时观测到同一群体在不同策略下的两种潜在结果，无法决定哪个策略胜出，需要构建一个反事实（Counterfactual）用来代表接受策略B的群体在接受A策略时的潜在结果。
具体来讲，构建一个与实验组群体特征均值无差异的对照组，用其观测结果代表实验组群体在施加A策略时的潜在结果，此时两种结果的均值差便是策略效应大小。由于是基于样本的观测数据得出的结论，需要通过显著性分析（Significance Test），以证明结论具有统计意义，这便是策略评估的完整路径。
根据能否在实验前控制策略的分配，我们将实验分为AB实验和观察性研究（Observational Studies），在AB实验分支下，根据能否控制策略的随机分配，又将AB实验分为随机对照实验（Randomized Experiments）和准实验（Quasi Experiments）。不同的实验类型使用不同的分组方法，在一定程度上影响着实验后分析数据的表现形式，实验后选择与实验类型匹配的分析方法尤为重要，直接制约着我们能否统计意义上的科学结论。具体分类如下：
对于大部分的实验场景，我们可以在实验前控制对不同的实验对象分配不同的策略，然而在有些场景下，如：①测试线上演唱会活动对短视频平台的影响，考虑到用户公平，需要给全部用户施加演唱会活动策略；②在测试不同的营销邮件策略对用户影响的场景中，我们无法控制哪些用户会最终接受策略。我们要么不能控制策略分配，要么不能控制策略在对应的人群生效，只能采用观察性研究，即在自然状态下对研究对象的特征进行观察、记录，并对结果进行描述和分析。
在我们可以控制对实验对象施加策略的场景，如①测试不同的产品UI对用户的影响，进而决定使用哪种UI；②快速验证首页商品列表图素材对转化率的影响。这些典型的C端实验场景，不仅有海量用户且用户在实验组、对照组间的行为不会相互影响，可以通过随机分组的方式找到同质且独立的实验组和对照组，这类实验称之为随机对照实验，是业界衡量策略效应的黄金标准。
然而在美团履约业务场景中，如调度场景，要测试不同的调度策略对区域内用户体验的影响，策略施加单位是区域，由于区域数量少，同时区域之间各项指标（商家、运力、消费者）差异较大，采用随机分组难以得出同质的实验组、对照组，而且由于区域之间可以共享运力，施加不同策略的实验组、对照组区域之间相互影响，不满足实验单位独立的条件。在这种场景下，我们不能对实验对象进行随机分配，只能有选择的进行实验组和对照组的分配，这种虽然能够控制策略分配但不能控制策略随机分配的实验，我们称之为准实验，常用的准实验方法如双重差分。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wfsui.github.io/posts/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BE%9B%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BF%A1%E7%9A%84ab%E6%B5%8B%E8%AF%95%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-09T02:39:55+00:00" />
<meta property="article:modified_time" content="2024-02-09T02:39:55+00:00" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="如何提供一个可信的AB测试解决方案"/>
<meta name="twitter:description" content="1 背景 虽然AB测试（AB实验）的统计基础已经有一个世纪的历史了，但大规模地构建一个正确可靠的A/B测试平台仍然是一个巨大的挑战：不仅要在实验设计环节应对溢出效应和小样本的双重挑战，平衡好实验偏差与方差以确定合适的实验单元、分组方法和分析方法，给出合理的实验设计，而且要在分析环节应对方差计算、P值计算、多重比较、混淆因素、假阴性（实际策略有效果，但是检测显示无效果）等多种统计陷阱。因此，要获得高质量的结果需要对实验和统计有专家级的理解，这无疑增加了实验门槛，难以达成任何人进行实验都可得出可信结论的目标。
本文将从实验方法和平台建设的两个视角，分别介绍如何正确地使用统计方法避免统计陷阱，以及输出什么样的平台能力，从而确保任何人使用该平台时都可得出可信结论。同时，我们也积累了如何进行更好的实验，以及如何利用实验来做出更好的决策，希望能给从事相关工作的同学有所帮助，也真诚地希望欢迎大家给出反馈或者建议，不断优化我们的工作。
2 走进AB测试 哪个线上选项会更好？我们经常需要做出这样的选择。当我们想要在两个策略之间做出决定时，理想的方案是面向同一拨用户，在两个平行时空，平行时空1体验原策略A，平行时空2体验新策略B，然后根据观测到的事实进行比较，以决定哪个策略胜出。然而在现实世界中，不存在两个平行时空，针对同一用户，我们只能观察到其接受策略A或策略B的一种效果，即反事实结果是观测不到的。
因此，在现实世界中，我们通常采用实验的方法做出决策。它将用户分配到不同的组，同一组内的用户在实验期间使用相同的策略，不同组的用户使用不同的策略。同时，日志系统根据实验系统为用户打标记，用于记录用户的行为，然后根据带有标记的日志计算度量差异，并进行统计分析以排除由于噪声导致的任何差异。实验者通过这些指标去理解和分析不同的策略对用户起了什么样的作用，是否符合实验预先假设。
2.1 AB测试概述 实证中由于不可能同时观测到同一群体在不同策略下的两种潜在结果，无法决定哪个策略胜出，需要构建一个反事实（Counterfactual）用来代表接受策略B的群体在接受A策略时的潜在结果。
具体来讲，构建一个与实验组群体特征均值无差异的对照组，用其观测结果代表实验组群体在施加A策略时的潜在结果，此时两种结果的均值差便是策略效应大小。由于是基于样本的观测数据得出的结论，需要通过显著性分析（Significance Test），以证明结论具有统计意义，这便是策略评估的完整路径。
根据能否在实验前控制策略的分配，我们将实验分为AB实验和观察性研究（Observational Studies），在AB实验分支下，根据能否控制策略的随机分配，又将AB实验分为随机对照实验（Randomized Experiments）和准实验（Quasi Experiments）。不同的实验类型使用不同的分组方法，在一定程度上影响着实验后分析数据的表现形式，实验后选择与实验类型匹配的分析方法尤为重要，直接制约着我们能否统计意义上的科学结论。具体分类如下：
对于大部分的实验场景，我们可以在实验前控制对不同的实验对象分配不同的策略，然而在有些场景下，如：①测试线上演唱会活动对短视频平台的影响，考虑到用户公平，需要给全部用户施加演唱会活动策略；②在测试不同的营销邮件策略对用户影响的场景中，我们无法控制哪些用户会最终接受策略。我们要么不能控制策略分配，要么不能控制策略在对应的人群生效，只能采用观察性研究，即在自然状态下对研究对象的特征进行观察、记录，并对结果进行描述和分析。
在我们可以控制对实验对象施加策略的场景，如①测试不同的产品UI对用户的影响，进而决定使用哪种UI；②快速验证首页商品列表图素材对转化率的影响。这些典型的C端实验场景，不仅有海量用户且用户在实验组、对照组间的行为不会相互影响，可以通过随机分组的方式找到同质且独立的实验组和对照组，这类实验称之为随机对照实验，是业界衡量策略效应的黄金标准。
然而在美团履约业务场景中，如调度场景，要测试不同的调度策略对区域内用户体验的影响，策略施加单位是区域，由于区域数量少，同时区域之间各项指标（商家、运力、消费者）差异较大，采用随机分组难以得出同质的实验组、对照组，而且由于区域之间可以共享运力，施加不同策略的实验组、对照组区域之间相互影响，不满足实验单位独立的条件。在这种场景下，我们不能对实验对象进行随机分配，只能有选择的进行实验组和对照组的分配，这种虽然能够控制策略分配但不能控制策略随机分配的实验，我们称之为准实验，常用的准实验方法如双重差分。"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://wfsui.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://wfsui.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://wfsui.github.io/css/dark.css" media="(prefers-color-scheme: dark)" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="https://wfsui.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<base href="https://wfsui.github.io/">
	<h1 class="site-title"><a href="https://wfsui.github.io/">大峰哥</a></h1>
	<div class="site-description"><h2>记录日常生活哦</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/wfsui" title="Github"><i data-feather="github"></i></a><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">首页</a>
			</li>
			
			<li>
				<a href="/posts">文章</a>
			</li>
			
			<li>
				<a href="/about">关于</a>
			</li>
			
			<li>
				<a href="/tags">标签</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">如何提供一个可信的AB测试解决方案</h1>
			<div class="meta">Posted at &mdash; Feb 9, 2024</div>
		</div>

		<div class="markdown">
			<h2 id="1-背景">1 背景</h2>
<p>虽然AB测试（AB实验）的统计基础已经有一个世纪的历史了，但大规模地构建一个正确可靠的A/B测试平台仍然是一个巨大的挑战：不仅要在实验设计环节应对溢出效应和小样本的双重挑战，平衡好实验偏差与方差以确定合适的实验单元、分组方法和分析方法，给出合理的实验设计，而且要在分析环节应对方差计算、P值计算、多重比较、混淆因素、假阴性（实际策略有效果，但是检测显示无效果）等多种统计陷阱。因此，要获得高质量的结果需要对实验和统计有专家级的理解，这无疑增加了实验门槛，难以达成任何人进行实验都可得出可信结论的目标。</p>
<p>本文将从实验方法和平台建设的两个视角，分别介绍如何正确地使用统计方法避免统计陷阱，以及输出什么样的平台能力，从而确保任何人使用该平台时都可得出可信结论。同时，我们也积累了如何进行更好的实验，以及如何利用实验来做出更好的决策，希望能给从事相关工作的同学有所帮助，也真诚地希望欢迎大家给出反馈或者建议，不断优化我们的工作。</p>
<h2 id="2-走进ab测试">2 走进AB测试</h2>
<p>哪个线上选项会更好？我们经常需要做出这样的选择。当我们想要在两个策略之间做出决定时，理想的方案是面向同一拨用户，在两个平行时空，平行时空1体验原策略A，平行时空2体验新策略B，然后根据观测到的事实进行比较，以决定哪个策略胜出。然而在现实世界中，不存在两个平行时空，针对同一用户，我们只能观察到其接受策略A或策略B的一种效果，即反事实结果是观测不到的。</p>
<p>因此，在现实世界中，我们通常采用实验的方法做出决策。它将用户分配到不同的组，同一组内的用户在实验期间使用相同的策略，不同组的用户使用不同的策略。同时，日志系统根据实验系统为用户打标记，用于记录用户的行为，然后根据带有标记的日志计算度量差异，并进行统计分析以排除由于噪声导致的任何差异。实验者通过这些指标去理解和分析不同的策略对用户起了什么样的作用，是否符合实验预先假设。</p>
<p><img src="https://p0.meituan.net/travelcube/7ac2b4641bc76670093a6380c37ed100225654.png" alt="图1 理想和现实中的策略评估"></p>
<h3 id="21-ab测试概述">2.1 AB测试概述</h3>
<p>实证中由于不可能同时观测到同一群体在不同策略下的两种潜在结果，无法决定哪个策略胜出，需要构建一个反事实（Counterfactual）用来代表接受策略B的群体在接受A策略时的潜在结果。</p>
<p>具体来讲，构建一个与实验组群体特征均值无差异的对照组，用其观测结果代表实验组群体在施加A策略时的潜在结果，此时两种结果的均值差便是策略效应大小。由于是基于样本的观测数据得出的结论，需要通过显著性分析（Significance Test），以证明结论具有统计意义，这便是策略评估的完整路径。</p>
<p>根据能否在实验前控制策略的分配，我们将实验分为AB实验和观察性研究（Observational Studies），在AB实验分支下，根据能否控制策略的随机分配，又将AB实验分为随机对照实验（Randomized Experiments）和准实验（Quasi Experiments）。不同的实验类型使用不同的分组方法，在一定程度上影响着实验后分析数据的表现形式，实验后选择与实验类型匹配的分析方法尤为重要，直接制约着我们能否统计意义上的科学结论。具体分类如下：</p>
<p><img src="https://p1.meituan.net/travelcube/5700fc456fddd2102cb734320fcd79ed234193.png" alt="图2 履约业务下的三种实验类型"></p>
<p>对于大部分的实验场景，我们可以在实验前控制对不同的实验对象分配不同的策略，然而在有些场景下，如：①测试线上演唱会活动对短视频平台的影响，考虑到用户公平，需要给全部用户施加演唱会活动策略；②在测试不同的营销邮件策略对用户影响的场景中，我们无法控制哪些用户会最终接受策略。我们要么不能控制策略分配，要么不能控制策略在对应的人群生效，只能采用观察性研究，即在自然状态下对研究对象的特征进行观察、记录，并对结果进行描述和分析。</p>
<p>在我们可以控制对实验对象施加策略的场景，如①测试不同的产品UI对用户的影响，进而决定使用哪种UI；②快速验证首页商品列表图素材对转化率的影响。这些典型的C端实验场景，不仅有海量用户且用户在实验组、对照组间的行为不会相互影响，可以通过随机分组的方式找到同质且独立的实验组和对照组，这类实验称之为随机对照实验，是业界衡量策略效应的黄金标准。</p>
<p>然而在美团履约业务场景中，如调度场景，要测试不同的调度策略对区域内用户体验的影响，策略施加单位是区域，由于区域数量少，同时区域之间各项指标（商家、运力、消费者）差异较大，采用随机分组难以得出同质的实验组、对照组，而且由于区域之间可以共享运力，施加不同策略的实验组、对照组区域之间相互影响，不满足实验单位独立的条件。在这种场景下，我们不能对实验对象进行随机分配，只能有选择的进行实验组和对照组的分配，这种虽然能够控制策略分配但不能控制策略随机分配的实验，我们称之为准实验，常用的准实验方法如双重差分。</p>
<p>随机对照实验，因为其能够保证实验组、对照组两组的特征均值相同，不会因为分组差异干扰对真实效应的衡量，是业界衡量策略效应的黄金标准。在不满足随机对照实验约束的业务场景下衡量策略效应，我们采用准实验的方法，通过改进分组方法消除实验组、对照组可观测特征的差异或使其保持恒定差异，分析环节采用适配准实验场景的分析方法。</p>
<p>如果由于场景约束，只能基于实验后得到的数据来进行实验的话，就只能采用适用于观察性研究的方法。准实验和观察性研究虽然不是衡量策略效应的金标准，但是如果使用得当，也可以得出相对科学可信的分析结论。在学界，三种不同实验类型的可信度等级如下：</p>
<p><img src="https://p1.meituan.net/travelcube/3ecf6056d573e8852ab3137cd6ba908b52059.png" alt="图3 用于评估AB测试质量的可信度等级"></p>
<h3 id="22-ab测试的关键问题">2.2 AB测试的关键问题</h3>
<p>不管何种类型的AB实验，都符合分流-&gt;实验-&gt;数据分析-&gt;决策的基本流程，以及需要满足AB实验的3个基本要素。分流是实验平台的顶层设计，它规范和约束了不同实验者如何在平台上独立运行各自实验而不相互影响，运行实验，看似简单，但是成功运行不同类型实验的前提是实验场景要满足其理论假设。</p>
<p>AB实验主要是通过观察抽样的样本来推断总体的行为，属于预测型结论，数据分析涉及大量的统计学理论，稍有不慎，容易掉入统计陷阱。上述流程，任一环节出错，都可能导致错误的结论，因此，AB实验统计一个数字容易，得到可靠可信的统计结论并不容易。</p>
<p><img src="https://p0.meituan.net/travelcube/4f204027cc0e8288c4d2d212b404bef7112218.png" alt="图4 构建可信AB测试的关键要素"></p>
<h4 id="221-ab测试的分流框架">2.2.1 AB测试的分流框架</h4>
<p>在履约技术平台，我们通过实验衡量真实的用户反应，以确定新产品功能的效果，如果无法同时运行多个并行实验，将会大大减慢迭代速度。扩大同时运行实验的数量对于实现更快的迭代是必不可少的。为了增加可以同时运行的实验数量，提高并行性，并允许同时运行多个互斥实验，业界出现了两种分流框架，一种是像谷歌、微软、脸书这种单边业务形态的公司，采用层、域嵌套的分流框架；另一种是像Uber、DoorDash这种多边业务形态的公司，采用基于约束的分流框架。具体如下图所示：</p>
<p><img src="https://p1.meituan.net/travelcube/cfe51bc8b77266a7219996a9d17141b1582830.png" alt="图5 业界流行的两种分流框架"></p>
<p><strong>基于层、域嵌套的重叠分流框架</strong>：该分流框架的特点是事先将流量随机打散做桶号标识并提前规划流量的用途，如上图所示，提前将全国流量划分为10等份并用1到10桶号来标识流量，1-6号桶的流量用于短期策略验证，7-10号桶用于长期性策略验证。为了支持同时运行多个互斥实验，提升迭代效率，分别在1-6号桶和7-10号桶中，进一步区分了正交桶和互斥桶，落在正交桶中的流量可以同时进入多个实验，在每进入一个实验前，重新打散一次流量，避免上一实验的残留效应做下一实验造成影响，实现了多个互斥实验的并行运行，落在互斥桶的流量，每次只能进入一个实验，用于运行不满足随机打散条件的实验。基于桶号划分的用于特定用途的一组流量集合，我们称为域；同一流量进入的不同类型的实验，我们称为层。</p>
<p>该分流框架的优点是不仅可以实现流量复用，扩大实验并行度，而且较容易避免具有潜在交互作用的实验可能给用户带来的糟糕体验。引入层的概念，将系统参数划分为多个层，约束让组合在一起可能产生较差用户体验的实验必须位于同一层，同一用户可进入不同层多个实验以实验流量复用，并防止其进入同一层多个实验，避免糟糕的用户体验。</p>
<p><strong>不足之处在于</strong>：首先，这种分流框架的一大前提是提前将流量打散，这种在大流量的单边场景下尚可，在小流量的多边场景下，难以行得通。多边场景下，考虑到溢出效应，无法直接采用单边实体进行分流，而是采用聚类的方式将有相互作用的多边聚合成一个大的实体，基于大的实体进行分流，考虑到有限的实体数量，这种提前打散的方式难以获得均匀的流量；其次，域提前规定了流量用途，这种提前隔离的方式，降低了流量利用率，无法满足小流量场下实验功效要求，如即使在互斥域中没有实验，也无法采用该流量进行其它的正交实验；第三，这种提前预规划流量用途的分流框架灵活性不足，如果后期发现域的设置不合理，要改变域的配置将付出较大的代价。</p>
<p><strong>基于冲突检测的分流框架</strong>：该分流框架的特点是由实验者制定约束，平台根据实验者制定的约束，确保无法避免潜在交互影响的实验没有同时曝光给用户。如微软、Uber等公司，实验平台都集成了检测交互作用的自动化系统，以避免实验间潜在交互影响。以Uber为例，将策略看作是一组独立参数的集合，并提前声明对应策略涉及的专用参数和与其它策略共享的参数，配置实验时检测是否有任何影响相同参数的两个实验重叠，只要它们没有重叠，便允许实验创建或更新操作。</p>
<p>该分流框架的优点是灵活且能最大限度的复用流量，相较于重叠流量框架，不受提前划分的域的约束只能在特定域中进行实验，即使对应的域中此时并无其它实验。只要满足进行并行实验的条件，便可以任意圈定流量进行实验；不足之处在于：实验平台需要构建自动检测交互作用的能力。</p>
<h4 id="222-ab测试满足的基本要素">2.2.2 AB测试满足的基本要素</h4>
<p>在运行AB实验时，要满足三个基本要素：①施加不同策略的实验组、对照组可比较，即实验组、对照组的特征均值在实验前相同或在实验前具有固定差异，便于实验后计算哪些差异是由于策略不同导致的；②策略之间没有干扰，实验群体相互独立，即当我们比较策略A和策略B时，接受策略A的用户行为不会受接受策略B的用户行为影响；③实验群体的数量要足够，以满足功效要求，避免实验结果假阴性，即实际策略有效果但是由于样本量不足没有检测出来。</p>
<p>不满足要素一，实验后难以确定实验组对照组的差异是由策略导致的还是由于分组导致的，难以准确衡量策略的真实效应；不满足条件二，可能会高估策略效应。举个例子，在履约配送范围实验中，橘色是实验组，商户A范围的扩大，会使用户的需求从商户B转移到商户A，如果评估的是商户集合的单量，会造成实验组单量相对对照组多，实验环节，得出扩大配送范围，会提高整体单量，但是当策略在全国应用后，发现单量并没有明显增加，因为实验期间观察的增加仅仅是单量转移，实验组单量转移到了对照组。不满足条件三，难以确定策略无效果是真正的无效果还是由于样本量不足，没有检测到策略效果。</p>
<p><img src="https://p0.meituan.net/travelcube/db9ac728ca40faeb6d0e67bcf988e2b1912693.png" alt="图6 溢出效应导致的估计偏差举例"></p>
<h4 id="223-不可忽视的统计陷阱">2.2.3 不可忽视的统计陷阱</h4>
<p>AB实验主要是通过观察抽样的样本来推断总体的行为，属于预测型结论，涉及大量的统计学理论，稍有不慎，容易掉入统计陷阱，难以得出可靠的统计结论。</p>
<p>实验组和对照组之间的差异是真实的还是噪音通过显著性检验来辅助判断，要得出结论涉及方差、检验方式和P值计算，这些环节充斥着统计陷阱，稍有不慎便会导致我们通过假设检验得到错误的结论。样本的抽样方式、分布特点以及样本量大小决定了我们的检验方式和采用的具体P值计算方法，实验单元、分析单元和实验组、对照组差值类型，决定了方差计算，方差作为P值计算的一个输入，直接影响着P值结果。上述环节，忽略任一因素，会导致P值计算错误，使我们通过假设检验得到错误的结论。</p>
<p><img src="https://p0.meituan.net/travelcube/201d11056a3302ea49f99e7c89767e69185850.png" alt="图7 分析环节影响实验结论的因素"></p>
<p><strong>容易忽视的方差计算陷阱</strong>：如果不能正确的估计方差，那么P值和置信区间都将是错的，这些错误会导致我们通过假设检验得到错误的结论。高估的方差会导致假阴性，而低估的方差会导致假阳性。下面是几个估计方差时的常见错误。</p>
<p>样本在实验组或对照组的分配机制影响着其方差的计算，避免用随机抽样（样本满足独立同分布）下的样本方差计算方法计算非随机抽样下的样本方差。该陷阱主要出现在用随机对照实验下的方差计算方法计算准实验下的方差，导致方差计算错误。计算$Y_t-Y_c$的方差，按照方差性质，非随机抽样下，若$Y_t$和$Y_c$不独立：</p>
<p>$D(Y_t-Y_c)=D(Y_t)+D(Y_c)-2Cov(Y_t,Y_c)$</p>
<p>在随机抽样下，$Y_t$和$Y_c$独立：</p>
<p>$D(Y_t-Y_c)=D(Y_t)+D(Y_c)$</p>
<p>两者不相等。例如，在按天交替轮转的轮转实验中，一旦确定了实验开始第一天在实验组还是对照组，后续其它天在实验组和对照组相继确定。此时，实验组、对照组两组样本不独立，如果按照独立的方法计算方差，将错误估计方差。实际上，样本的分配机制影响着其方差计算，在AB测试中，我们将流量划分为实验组和对照组并在实验组施加策略，然后计算实验组相对对照组的某个度量指标的绝对提升值或者相对提升率，并检验该差异是否存在统计上的显著性，进而判断实验策略是否真实有效。</p>
<p><img src="https://p0.meituan.net/travelcube/710258959017d2c77f31560449f3bb5a269908.png" alt=""></p>
<p>从上述公式可以看出，方差计算与分配机制有关，如果忽略分配机制将导致错误的方差计算。</p>
<p><strong>评估相对提升或实验单元与分析单元不一致时，错误的方差计算方式容易低估实际方差，导致假阳性</strong>。在计算指标的相对提升率，如下公式所示：</p>
<p><img src="https://p0.meituan.net/travelcube/33e69e4d7375df70bd6ba6025b61daf3292457.png" alt=""></p>
<p><strong>容易忽视的检验方式导致的P值计算陷阱</strong>：统计学对于多大样本量即可认为中心极限定理成立并没有完全的定论，并非所有大样本场景下的样本分布都满足正态性假设，避免有偏样本采用默认正态分布下的检验方法。Weich t假设检验是参数检验常用的一种检验方法，其本质上假定实验组、对照组样本均值等的渐近正态性成立，该理论实际上是建立在大样本情形下的中心极限定理基础上。统计学对于多大样本量即可认为中心极限定理成立并没有完全的定论，这实际上也取决于原始分布本身偏离正态分布的程度。</p>
<p>从经验来看，若样本仅稍微偏离正态总体，大于30的样本量或许就足够了。然而对于有偏样本，<a href="https://www.exp-platform.com/Documents/2014%20experimentersRulesOfThumb.pdf">Ron Kohavi等(2014)</a>  指出当样本偏度大于等于1时，一个经验准则便是只有计算样本均值的观测样本量大于$355 s^2$时才可认为中心极限定律成立。实际抽取了一个样本量为13832的活动实验，其实验组、对照组差值的抽样分布呈现右偏，不符合正态分布，如下图所示：</p>
<p><img src="https://p0.meituan.net/travelcube/ff79baf17347c1207351f42a7feb8efb107288.png" alt="图8 数据分布的偏态举例"></p>
<p>如果所有场景下默认采用正态分布情形下的检验方式计算P值，容易导致错误的P值计算。</p>
<h3 id="23-基于一组核心抽象的平台建设难以适配所有业务场景">2.3 基于一组核心抽象的平台建设难以适配所有业务场景</h3>
<p>整个AB实验的过程涉及大量统计学知识，正确运用书本理论知识的前提是实际业务场景满足理论假设，实际情况是很多场景不满足理论假设。在这种情况下，获得高质量的结果需要对实验和统计有专家级的理解，以及大量的工作包括：实验设计、配置、指标加工、自定义分析等流水线工作，任一环节出错，将会导致大量的工作浪费。</p>
<p>通过核心抽象为实验者输出不同方法能力的平台建设思路，难以避免方法使用不当导致的实验置信问题，在此过程中任一设计的微小偏差，会导致无法比较的实验组、对照组，从而影响实验结果。例如，事例一方差估计错误：在实验分析时，经常犯的一个错误，不管分组方式是不是随机分组，在实际分析时，仍然按照样本满足独立同分布的条件计算方差，造成我们对估计值的准确性过度自信，低估了方差，容易犯假阳性的错误。</p>
<p>一个极端的例子，随机抽取100个学生用于估计该需要的平均成绩，如果被抽查的100人都是同一个学生，他们的成绩只反映了一个学生的成绩，对于估计所有学生平均成绩的信息含量等同于一个学生提供的信息。</p>
<p>如果我们把它们当成独立的，所得的样本平均值的标准误差显然是不对的，其结果是造成我们对估计值的准确性过度自信，即估计值的标准误差估计过小。</p>
<p>事例二业务场景不满足理论约束：双重差分是我们准实验中常用的分析模型，它的计算过程很简单，即实验组干预前后的均值的差减去对照组干预前后均值的差，根据业务场景，可以选择传统DID模型或固定效应的DID模型，但具体哪种模型合适，需要进一步看，在当下的业务场景下，哪种模型满足平行趋势假设，即在没有干预的情况下，实验组和对照组指标的均值差异在不同时间内保持一致，在都满足平行趋势假设下，哪种模型更优？如果不进行严格的检验，将会导致有偏估计。如下是我们具体场景下的案例：</p>
<p><img src="https://p0.meituan.net/travelcube/259a563adb7a51dcb0025230de5bdc3d688054.png" alt="图9 准实验场景下选择双重差分模型需要考虑的因素"></p>
<p>虽然根据大致的场景特点，可以判断出采用双重差分模型，但是到底采用双重差分的哪个模型，还需要基于实际数据，做进一步验证和选择。根据上图所示，在当前的业务场景下，并不满足传统DID模型的平行趋势假设，如果贸然使用，会造成估计偏差，时间效应的双重差分模型和个体+时间效应的双重差分模型，虽然都满足平行趋势假设，但从实际置信区间看，后者因为考虑了策略对不同个体的差异，波动较小，估计结果更加接近实际值，所以应采用后者。</p>
<h2 id="3-在履约我们如何进行ab测试">3 在履约我们如何进行AB测试</h2>
<h3 id="31-多边业务模式下面临的ab测试难题">3.1 多边业务模式下面临的AB测试难题</h3>
<p>溢出效应和小样本是当前业务场景下实验面临的最大挑战，其次策略施加的公平性约束了实验分组也是我们不得不面对的挑战。每个因素的单独制约，对得出置信实验结论将是不小挑战，而履约场景下，这些因素综合的叠加在一起，加剧了挑战。</p>
<p>我们的即时配送物流系统在多边市场中扮演着交易中间人的角色，它通过平台匹配用户、骑手、商家三方的需求，平台通过产品策略优化这一匹配过程，每一次匹配都会对同一时刻及后续一段时间内其它的匹配产生影响，具有较强的溢出效应。受溢出效应影响，实验单元的实验结果不仅取决于个体本身，还会受其他实验单元的影响。网络效应的存在违反了实验单元独立的原则，导致有偏的实验结果。</p>
<p>如履约业务下的商家配送范围实验，实验Treatment（配送范围划分）直接决定了用户能否在某家商家下单，且同一时空下不同商家共享相同的用户，某一策略的施加会导致原本只能在商家B下单的用户可以在商家A下单，导致原本在对照组中的单转移到了实验组，虽然从实验效果上看，策略提升了单量规模，策略推全后，效果并未到达实验的先验预期甚至无效果。因为这种提升可能仅仅是单量转移，并非策略真正带来了提升。</p>
<p>履约LBS的业务形态决定了其大部分策略都是地域（主要是配送区域）展开的，受限于配送区域数量及本身的地域差异，难以获得足够的样本检测出策略的小提升。如调度实验，受限于自身业务形态和空间维度限制，调度算法的最小作用单元为区域或区域组，实验必须考虑区域或者更粗粒度的分流，然而大部分城市区域和区域组很少，并且城市各地域间的差异往往比较显著，这在数据上体现为区域间指标波动剧烈。</p>
<p>该场景下严峻的小样本与地域间差异显著的问题导致统计功效低，从而很难有效地检测出策略小的提升效果。其也会导致随机分流下与响应变量相关的协变量在实验组、对照组的分布差距较大，放大业务上实验组对照组不同质问题的同时给实验结果带来质疑。</p>
<p>更致命的是，该场景下的混合调度模式，不同运力类型的重叠区域可以共享运力互派单，区域可以召回其附近的其它区域运力并派单的特点带来的溢出效应，会导致实验效果估计不够精确甚至带来显著的估计偏差。类似调度实验同时要克服小样本和溢出效应的双重约束，是不小的挑战。</p>
<p><img src="https://p0.meituan.net/travelcube/ffcf13e31d7629b27cb13495cdfa57e7748076.png" alt="图10 履约业务模式下面临的AB测试难题"></p>
<h3 id="32-ab测试的组织和流程">3.2 AB测试的组织和流程</h3>
<p>随着履约业务的发展，我们越来越依赖于良好的策略驱动业务规模快速发展，以及效率、体验和成本的持续优化。A/B测试提供了最科学的方法来评估策略变化的影响，并绘制出清晰的因果关系。通过A/B测试量化影响，最终辅助团队做出决策。我们将人员、流程和平台更紧密地结合在一起——这是成功实验生态系统的基本要素。</p>
<p>人员方面，我们将算法（实验用户）、算法工程、数据科学（下称数科）有机组合成了一个虚拟团队，数科同学在策略迭代之初，就参与到算法年度目标的讨论中，辅助算法一起制定量化策略好坏的综合评估指标，并基于场景特点选择合适的实验方法，完成对应场景下的实验设计，算法工程同学，负责将新方法集成到实验平台，作为公共能力为用户提供服务。</p>
<p><img src="https://p1.meituan.net/travelcube/e7fe41e4a52f57c590f26afde1bd7ab990355.png" alt=""></p>
<p>有了组织、平台后，构建高效、基于AB实验的数据驱动工作流是通过AB实验帮助我们达成产品目标获得成功的关键，我们将整个流程分为三个阶段：构建想法，通过AB实验验证想法、沉淀知识库形成实验记忆。</p>
<p>构建想法是实验的输入阶段，构建想法的质量直接决定了实验的效果，如果这个阶段构建的想法不够好，那么AB实验阶段只能起到验证错误的作用，降低犯错误的概率，无法带来增长。</p>
<p>验证想法就是实践AB实验的过程，可以分为实验假设、实验设计、实验运行、实验分析和实验决策五个关节，实验假设环节，即形成实验目标，构建综合评估指标，实验设计，基于场景约束，选择合适的实验方法。</p>
<p>最后，通过Launch Review发起实验决策；将成功和失败的案例沉淀下来，形成实验记忆，不仅可以帮助我们发现策略的通用性，而且有助于帮助我们从失败中寻找机会。</p>
<p><img src="https://p0.meituan.net/travelcube/45e5509a0a6f7702397b9e16eba1151d251373.png" alt="图11 履约AB测试流程"></p>
<h3 id="33-ab测试平台简介">3.3 AB测试平台简介</h3>
<h4 id="331-平台概述">3.3.1 平台概述</h4>
<p>AB实验得以在工程中广泛应用和推广，与AB实验的并行性（多个实验可并行开展）和先验性（通过小流量预先获得效果评估）密不可分，分流框架直接决定了实验的并行度，与场景匹配的实验解决方案直接影响着实验先验结论的可信度。</p>
<p>为提高实验并行度并让我们同时运行多个互斥的实验，我们构建了基于约束的分流量框架，以规范和约束不同实验如何共享和使用流量。为确保平台提供可靠的实验结果，平台针对实验设计直接输出解决方案而非能力，实验分析完全自动化，即基于实验设计和数据特点自适应选择与之匹配的方法。以达成不管是实验或统计领域的专家进行实验，还是无统计和实验知识的普通用户进行实验，任何人都能够相信实验的结果。</p>
<p>为提高实验并行度，业界有层、域嵌套的重叠分流框架和基于约束的分流框架，前者以谷歌为代表，后者以Uber、微软为代表。层、域嵌套的重叠分流框架要求提前将流量均匀打散并规划好用途，这不仅需要大流量以确保能够被均匀打散，而且对业务未来演进需要有精准预判确保合理划分流量用途。小流量无法实现均匀打散，流量用途划分不合理，不仅会导致分配流量大的域实验数量少而浪费流量，分配流量小的域实验数量多导致流量不够用而排队；而且重新划分流量用途会导致线上实验失效、新策略无法正确推全、无法进行长期实验。</p>
<p>在履约技术平台，分流单位经常是区域、区域组甚至城市，样本量有限，不满足均匀打散的样本量要求，其次，履约业务持续演进变动，难以基于业务预判提前规划好流量用途，基于以上两点，履约采用了基于约束的分流框架。</p>
<p>在实验领域使用统计方法统计一个数字容易，但是确保统计方法合理适配得出可靠的实验结论并不容易，特别是在履约这种连接用户、骑手、商家三边的平台型经济业务模式下，不同的实验需要在降低网络效应和提高实验功效两个目标之间权衡，制定与场景匹配的实验方法并得出置信实验结论。</p>
<p>做到这一点，需要实验者对实验和统计有专家级的理解。为降低实验门槛，确保实验置信，平台建设提出了，针对实验设计直接输出解决方案而非能力。针对实验分析，实现完全自动化，避免实验者将大量精力放在方案的论证上和人为因素导致的实验置信问题。</p>
<h4 id="332-基于约束的分流框架以适配履约业务场景">3.3.2 基于约束的分流框架，以适配履约业务场景</h4>
<p>分流框架像法律法规规范着规范着大家的日常行为，使大家在社会大家庭中有序生活一样，它规范和约束着不同的实验在不相互影响的前提下如何共享和使用流量，它是实验平台的顶层设计。基于约束的分流框架让实验者指定约束，平台冲突检测根据实验者指定的约束，进一步判断是否允许实验。在展开之前，先引入三个概念：算法Key、场景和实验模版。</p>
<p>算法Key代表一组可独立测试的功能，在技术层面可以表示为一组独立参数集合，场景代表对应算法Key（对于联合实验而言，是多个算法Key；非联合实验，是一个算法Key）下具有相同实验模版的实验集合，实验模版为一组相同实验类型、实验单位、分组方法、评估方法的配置。</p>
<p>考虑到：①同一算法Key，不同实验是针对同一功能不同版本的测试，实验间要互斥；②不同算法Key之间，只要其对应的功能之间没有潜在的交互作用，其对应的实验间天然正交可以放心的复用流量，如存在潜在交互作用，只要确保流量能被随机打散，便可消除策略间的潜在相互作用对实验结论的影响。</p>
<p>因此，针对并行实验，初步的约束如下：①同一算法Key下的任意两个实验不能复用流量，冲突；②不同算法Key下存在潜在交互作用的两个实验，只要有一个实验类型是随机对照实验，皆可复用流量。约束②不仅避免了全因子流量框架不同策略实验间潜在相互影响的风险，而且避免了重叠流量框架因不同域流量隔离导致的流量复用率低的问题，特别是在准实验、观察性研究比随机对照实验多的情形下，由于准实验、观察性研究分处不同的域，无法实现随机对照实验和准实验、观察性研究之间的流量复用。</p>
<p>考虑到同一算法Key下不同实验因目标流量或迭代验证的功能不同，同一个算法Key下的不同实验与另一算法Key下的不同实验间是否冲突取决于其对应的测试功能或实验方法，我们引入场景来描述不同算法Key的功能描述和其对应的实验方法，并根据业务经验构造不同场景间的业务影响矩阵。基于不同场景的业务影响矩阵、场景实验方法和并行实验约束，生成场景实验冲突矩阵，基于此矩阵完成不同算法Key实验间的冲突检测。</p>
<p><img src="https://p0.meituan.net/travelcube/71885b1b71adb97dcb3c3bfa10a2339c344394.png" alt="图12 履约实验平台分流框架"></p>
<p>不同场景下的联合实验，与其对应算法Key下所有场景实验冲突，与其它算法Key场景实验，根据约束2进一步判定；为避免冲突实验间的流量重叠，提供了基于表达式定义流量范围的能力，通过检测表达式流量覆盖范围避免冲突实验间的流量重叠。基于约束的分流框架，不预先规划流量用途，也没有层、域复杂概念，实验时按需选择流量，只要通过冲突检测，就可以上线实验，不仅降低了用户使用门槛，而且提高了平台灵活性，以适应履约业务场景。</p>
<h4 id="333-打包输出实验设计降低实验门槛确保实验质量">3.3.3 打包输出实验设计，降低实验门槛，确保实验质量</h4>
<p><strong>受履约溢出效应、小样本和公平性因素制约，实验设计是降低溢出效应、提高实验功效、关注实验公平等多种目标进行方差和偏差平衡的过程</strong>。</p>
<p>实验者虽然可以根据对应场景所允许的分组方式（是否允许分组以及在允许分组的前提下是否允许随机分组），初步判断可以考虑应采用如下“普通随机对照实验”、“随机轮转实验”、“准实验”、“观察性研究”中的哪一种实验方式，但是具体应该采用哪种实验方式以及对应实验方式下应该采用什么分流单元，是综合考虑溢出效应、实验功效、公平性等因素多方平衡的结果。</p>
<p>例如：在运单实验中，实验组、对照组运单可以来源于同一区域，由于同一区域的运单可以共享骑手，运单间不独立，导致实验组、对照组存在溢出效应。轮转实验是解决该问题的一个可选项，前提是我们需要在如下两个相互冲突的目标之间做平衡。</p>
<ul>
<li>我们希望可以划分更多的实验单元来增加样本量，这就需要我们将实验单元划的足够小以得出更多的实验单元来保障我们有更多的样本量来满足实验灵敏度要求。</li>
<li>我们希望实验单元划的足够大确保将相互影响的个体包含在一个独立单元中，以消除溢出效应对实验结果的影响。</li>
</ul>
<p><img src="https://p0.meituan.net/travelcube/efb932779645042c37f778123bad1be3775680.png" alt="图13 轮转实验-在样本量和溢出效应之间平衡"></p>
<p>在有限的样本下，如果只是进行简单的随机分组，不仅会导致实验组对照组的一些指标在实验前存在偏差，而且会由于样本量不足导致无法检测出策略的微小提升，我们到底是通过控制影响指标差异的协变量和改进分组方式来达成偏差和方差的平衡，还是实验前允许偏差存在，通过实验后纠偏的方法进行补充，这些都需要在实验设计时基于算力，以及基于分组方式和分析方法组合方案得出的数据表现综合判断，来制定合理的实验方案。</p>
<p><strong>为数科同学提供一系列实验方案的设计工具，辅助其完成实验设计中关键的方差和偏差平衡，输出与场景匹配的解决方案</strong>。由于用户、商家、骑手通过履约平台形成的复杂交互关系，在履约进行AB实验，需要去权衡溢出效应和实验功效，给出合理的实验设计，这不仅需要从事该工作的人对实验和统计有专家级的理解以及足够的时间投入，而且需要平台提供实验设计的能力，可按需选择实验类型、实验单位、分组方式、分析方法、评估指标，完成实验设计并验证该设计的可行性。</p>
<p>在履约技术部，由数科同学承担实验设计的职责，由其为对应场景制定与之匹配的实验方案，释放算法同学精力，让其有更多的时间思考如何进行策略迭代。为满足履约各种场景实验诉求，平台提供了如下几种类型实验模版：</p>
<p><img src="https://p1.meituan.net/travelcube/143cf55d7a9ee87de6b9fb9969459878272372.png" alt="图14 实验平台为数科同学推荐的实验设计模版"></p>
<p>为了便于数科同学基于要检测的指标、对应指标的预计提升量、可用样本等约束进一步确定对应实验模版下的实验单位、具体的分组方法和分析方法，制定出与场景匹配的实验方案，平台为其提供了实验设计工具包，包括：分组工具包、降方差工具包、显著性分析工具包，实验设计报告工具包，包括：MDE分析、同质性检验、样本量预估等工具包。</p>
<p><img src="https://p0.meituan.net/travelcube/a55dc573216749e2aa42f4f02fc7046c136625.png" alt="图15 实验设计工具包"></p>
<p><strong>平台为实验者直接输出与其场景匹配的实验设计方案，实验者无需担心实验方法使用不当导致的置信问题</strong>。在平台的场景管理模块，配置着该场景下具体的实验方案：具体的实验类型、实验单位、分组方式、评估指标（包括：目标指标、护栏指标、驱动指标）、降方差方法，实验者进行实验配置时，根据平台为其提供样本量预估和“MDE”分析工具，完成流量圈选和实验周期确定，之后平台输出实验设计报告，输出分组结果、“MDE”分析和同质性检验报告，检验通过后，实验者可进一步配置实验组、对照组对应的策略参数，完成最终的实验配置，进而发布实验。</p>
<p><img src="https://p0.meituan.net/travelcube/9d570200393faa2ccbf5b6955a89d4f2106863.png" alt="图16 平台基于实验设计方案和实验者的实验配置，为具体实验确定实验方案"></p>
<h4 id="334-构建适用于不同实验方法的分析引擎标准化实验分析">3.3.4 构建适用于不同实验方法的分析引擎，标准化实验分析</h4>
<p>可靠的数据和科学的分析方法是得出可信分析的关键。实验分析涉及大量的统计学理论，稍有不慎，容易掉入统计陷阱，考虑到大多数实验者缺乏统计知识且自助分析带来的时间消耗，我们构建了统一的分析引擎，标准化实验分析过程，为不同的实验设计提供与之匹配的分析方法，通过验证相关指标的统计显著性和估计策略效应，帮助我们根据分析结果做出数据驱动的决策。分析流程大致包括如下过程：</p>
<ol>
<li>通过数据诊断，确保分析数据的可靠性；</li>
<li>基于分组方式、分析方法，进行实验效应估计，得出实验效应估计；</li>
<li>基于分组方式、数据类型、实验单元与分析单元的关系、分析方法，选择合适的方差计算方式进行降方差以提高实验灵敏度，避免假阴性；</li>
<li>基于分组方式、数据分布特点，选择合适的检验方式计算方差计算和P值，验证相关指标的统计显著性给出统计结论；</li>
<li>基于诊断和分析，输出实验报告。</li>
</ol>
<p><img src="https://p0.meituan.net/travelcube/a36e0a9fdc2d29a3545d7e22b9eaa39c401307.png" alt="图17 履约实验平台统计引擎"></p>
<p>分析环节的数据诊断，旨在提醒实验者注意可能违反实验假设的情况。很多人认为实验一定按照设计运行，实际上这一假设失败的概率远高于人们的预期。失败实验的分析结论通常是有严重偏颇的，甚至一些结论是完全错误的。在输出显著性分析报告之前，通过护栏指标检验，确保业务不会因策略的迭代受到伤害，通过分组同质性检验、“SRM”检验查看实验执行是否符合预期，确保实验本身的可信度，抽样分布检验，为后续选择合适的显著性检验方法提供依据。</p>
<p>分析环节的实验分析，自动选择与数据和实验设计匹配的分析方法，避免统计陷阱。根据分组方式，提供了差值法和双重差分两种效应估计方法，方差计算和P值计算是统计陷阱集中发生的两个环节，分别提供了判别方差和P值计算方式的判别引擎。</p>
<p>首先，方差计算根据是否随机抽样，将其分为独立样本方差计算和非独立样本方差计算，独立样本方差计算，根据指标是增量提升还是相对提升和分流单位与分析单位是否一致综合因素，提供了直接计算和Delta方法计算，避免方差计算陷阱，非独立样本，通过模拟数据的实际分布，给出方差的准确计算。</p>
<p>其次，在样本量小于30的超小样本下，采用非参的Fisher检验以满足功效要求，在样本量大约1万的超大样本下，接受检验统计量的渐近正态性成立并采用Weich t假设检验；在样本量大于30小于1万下，进一步样本实际分布情况，如果统计量渐近正态性成立，则采用Weich t检验，如果不成立，采用Bootstrap估分布进行统计推断。</p>
<h4 id="335-平台建设不仅要根植于业务场景而且要做到严格的质量控制确保结果可信">3.3.5 平台建设不仅要根植于业务场景，而且要做到严格的质量控制，确保结果可信</h4>
<p>履约实验平台建设拒绝功能的简单堆砌，而是设计了可灵活扩展的实验Pipeline工程架构，然后将更多的精力放在业务场景的实验设计和分析方案上，严格控制质量，确保实验结果可信。在此过程中，数科同学扮演了双重角色，作为平台建设的一员，他们永远比工程建设快半步，深入业务，定义新问题并找到答案，协同工程同学一起完成新能力建设。</p>
<p>同时，数科同学又作为一批特殊的用户，承担者平台质量把控和产品易用性职责，特别是质量把控，在平台对外发布新实验方法前，都需要通过他们的AA模拟，通过模拟几百次AA实验，查看关注指标的P值是否在0到1之间均匀分布，通过验证后如果符合新能力发布条件，进行新能力发布，否则，继续分析找出问题所在。下表是在随机轮转实验中，引入入Fisher和Neyman检验时的模拟验证。</p>
<p><img src="https://p0.meituan.net/travelcube/47106d64c51f50ad3f41c4b97ff97b63251805.png" alt=""></p>
<h2 id="4-总结与展望">4 总结与展望</h2>
<p>在履约算法和业务同学每年分别运行数以万计的实验，测试内容涵盖履约业务的各方面，我们已经积累了如何进行更好的实验以及如何利用实验来做出更好的决策的知识。本文从实验通识的视角，介绍了履约在建设可信实验的实践，希望能对实验者有所帮助。由于履约的规模、影响力和多边业务模式的特色，履约的问题空间带来了独特挑战。溢出效应、小样本、策略公平性等综合因素制约着我们运行可信实验，我们在解决上述问题时，也沉淀了一系列实践，后续也会陆续推出相关实践文章。</p>
<p>基于不同的因素，如指标类型、样本量、样本分布特点等，可以应用不同的方法如线性模型、Delta方法、Bootstrap方法来计算P值和标准误差，并且在以城市、区域、站点为实验单位的实验中，可以自动选择不同的方法来调整标准误差，避免了由于数据聚类而导致的误报。这种灵活分析能力对策略的快速迭代非常重要，并且深受数据工程师的欢迎，这样他们就可以把精力和时间集中在实验的其他关键方面。</p>
<p>基于此，我们构建了统一的分析引擎，它标准化了实验核心框架，如普通随机对照实验、随机轮转实验、协变量自适应实验、准实验和观察性研究，以及一些其它的在产业界和学术界前沿的实验评估技术：如样本量预估、降方差、MED分析、数据纠偏、轮转实验的携带效应估计等，以减少大家花在分析上的时间。未来，我们进一步开放该能力，以服务于更多的用户。</p>
<h2 id="5-本文作者">5 本文作者</h2>
<p>王鹏、永斌、中锋等，均来自美团到家研发平台-履约平台技术部。</p>
<h2 id="招聘信息">招聘信息</h2>
<p><strong>履约平台技术部-数据科学工程师</strong></p>
<p>我们期望候选人可以建立统计模型、应用机器学习技术、分析配送业务数据，并使用这些建模技术构建相关指标来协助关键的业务决策。</p>
<ol>
<li>与算法或业务协作负责各类实验，确保实验的科学性与高效率；负责复杂实验的设计和评估，通过实验分析给业务决策提供推荐方案；负责科学实验评估平台的规划和演进。</li>
<li>和业务紧密协同，深入理解业务和产品，将业务和产品的问题转化为数据和技术的问题，并且设计合理的解决方案，如主动通过数据探索和挖掘，帮助业务自动识别虚假申诉并驱动其完善管控规则；通过体系化的因果推断解释日常业务核心指标的波动变化，及时发现问题等。</li>
<li>把握技术趋势加强行业对标调研适用于配送各业务的因果推断、统计推断，异常检测以及其它数据科学的方法，并把这些方法应用到实际业务问题。</li>
<li>指导团队成员和数据分析师，帮助他们快速成长，为团队培养数据分析人才。</li>
</ol>
<p>欢迎加入我们，简历请投递至：wangpeng47@meituan.com</p>

		</div>

		<div class="post-tags">
			
				
					<nav class="nav tags">
							<ul class="flat">
								
								<li><a href="/tags/%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F">美团技术团队</a></li>
								
								<li><a href="/tags/%E5%88%B0%E5%AE%B6">到家</a></li>
								
								<li><a href="/tags/%E6%95%B0%E6%8D%AE">数据</a></li>
								
								<li><a href="/tags/ab%E5%AE%9E%E9%AA%8C">AB实验</a></li>
								
								<li><a href="/tags/ab%E6%B5%8B%E8%AF%95">AB测试</a></li>
								
							</ul>
					</nav>
				
			
		</div>
		<div id="disqus_thread"></div>
<script type="text/javascript">
	(function () {
		
		
		if (window.location.hostname == "localhost")
			return;

		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		var disqus_shortname = 'wfsui';
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
		Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> © Copyright notice |  <a href="">Wfsui theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div><script>feather.replace()</script>
</body>
</html>
