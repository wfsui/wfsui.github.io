<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on 大峰哥</title>
    <link>https://wfsui.github.io/tags/transformer/</link>
    <description>Recent content in Transformer on 大峰哥</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Fri, 14 Jun 2024 02:47:34 +0000</lastBuildDate>
    <atom:link href="https://wfsui.github.io/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>基于多模态信息抽取的菜品知识图谱构建</title>
      <link>https://wfsui.github.io/posts/%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%E7%9A%84%E8%8F%9C%E5%93%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA/</link>
      <pubDate>Fri, 14 Jun 2024 02:47:34 +0000</pubDate>
      <guid>https://wfsui.github.io/posts/%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%E7%9A%84%E8%8F%9C%E5%93%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA/</guid>
      <description>1. 背景 中国有句古话：“民以食为天”。对食物的分析和理解，特别是识别菜肴的食材，在健康管理、卡路里计算、烹饪艺术、食物搜索等领域具有重要意义。但是，算法技术尽管在目标检测[1]-[3]、通用场景理解[4][5]和跨模态检索[6]-[8]方面取得了很大进展，却没有在食物相关的场景中取得好的表现，尤其是对烹饪菜肴的相关场景。其核心原因是缺乏细粒度食材的基准，这已经成为该领域发展的瓶颈。&#xA;以往的研究主要集中在食物层面的表征学习，如Food2K上的食物识别[9]-[12]，UNIMIB2016上的食物检测[13]-[15]。然而，这些方法忽视了菜肴中的食材组成，也不理解食材之间的上下文关系。相比之下，一系列的方法[16]-[18]运用Recipe1M的“食谱-图像”对，实现了跨模态的食谱检索[16]。&#xA;然而，由于缺乏食材边界框的标注，这种类型的研究只能通过三元组建模出整个食物图像和食谱文本之间的关联[16],[19],[20]。这种限制导致图像区域与食物的一系列食材之间存在模糊的匹配关系，产生虚假相关性[21]。综上，目前迫切需要一个细粒度的食材级基准，促进复杂的食品场景理解算法的发展，并支持细粒度的任务，如食材检测和跨模态食材检索。&#xA;在本研究中提出对于中餐进行理解这一新任务，旨在捕捉中餐图像中食材之间的语义关系，并建立了有关中国菜品理解的新基准。我们大致设定了中餐理解的两个任务：食材检测和食材检索。对于食材检测，目标是确定图像中特定食材的存在并提供精确的定位。对于食材检索，目标是探索不同食材组合与食品图像之间的细粒度对应关系。对中餐的理解扩展了食品相关任务的范围，在食品领域开辟了更广泛的应用。同时，食材的多样外观和它们错综复杂的语境关系，对中餐的理解提出了一个更大的难题。&#xA;为了进行中餐理解这一新任务，我们需要构建一个包含食材粒度标注的数据集。然而，由于中餐种类繁多、风格独特，因此在食材标注上面临着巨大的挑战。构建含中餐食材的细粒度跨模态数据集主要有三个难点。&#xA;首先，相同的食材有不同的名称。图1.1(a)说明了这种情况：“圣女果”和“小番茄”都是广泛使用的食材名称，它们是同一食材的不同名称，这样的情况使得我们需要花费更多的精力来清除数据集中的模糊标签以及其他噪声。 其次，同一植物类食材之间的图像存在细微差异，如“青菜”和“油菜”，“香菇”和“冬菇”，如图1.1(b)所示。这些情况对标注人员来说是相当具有挑战性的，他们需要从文本部分获得一些提示。此外，对于下游任务来说，基于视觉特征来区分它们也是相当具有挑战性的。 第三，由于烹饪方法的原因，中国菜肴的食材通常分散在图像中。如图1.1©所示，碎片化食材通常缺乏清晰的轮廓边界。此外，从图1.1(d)中可以看出，食品图像中的主要食材往往占据显著区域，这不可避免地削弱了辅助食材的语义信息。这使得在提取食材特征的同时，对辅助食材之间的上下文关系进行建模成为一个关键问题。 为了应对上述挑战并促进对中餐理解的研究，我们开发了一个名为CMIngre (Cross-Modal Ingredient-level Dataset) 的跨模态食材级数据集。该数据集旨在通过提供对食材及其关系的有价值的见解来增强对中国烹饪的理解。该数据集由来自三个不同来源的8,001张图像组成，即菜肴，食谱和用户生成内容（UGC）。该数据集包含429种不同的中国食材和95,290种食材边界框。&#xA;为了对广泛的食材进行全面的语义分类，我们根据中华人民共和国健康行业标准对食品食材数据表达的规定[23]，将其划分为更高级的层次。这些层次关系也可以作为先验信息，以促进在后续研究中探索不同食材之间的上下文关系。此外，我们评估了传统的基于CNN的检测算法和基于Transformer的预训练模型在CMIngre上食材检测任务的性能。我们还提出了食材检索任务的基线方法，该方法捕获单个食材的语义信息以及各种食材组合之间的关系，并进一步采用pooling策略来研究跨模态图像-食材之间的匹配关系。在CMIngre数据集上进行的深入实验评估证实了我们提出的方法在提高食材检测和检索性能方面的有效性。&#xA;本文的贡献可以概括为以下几点：&#xA;本文提出了一种新的基于“图像-文本”对的中餐理解任务，该任务扩展了细粒度对象检测和检索的范围，对中餐烹饪领域的理解提供进一步的帮助。 为了支持对中餐理解的研究，我们建立了一个名为CMIngre的跨模态食材级别的数据集，该数据集由来自三个不同来源的8,001组图像食材组成，涵盖了429种不同的中国食材和95,290个边界框。 我们评估了不同的目标检测算法在CMIngre数据集上的性能，并提出了跨模态食材检索任务的基线方法。 我们在CMIngre上对两个食材级的食品理解任务进行了广泛的实验，以评估我们提出的方法的有效性。 2.</description>
    </item>
  </channel>
</rss>
