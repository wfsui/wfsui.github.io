<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>测试 on 大峰哥</title>
    <link>https://wfsui.github.io/tags/%E6%B5%8B%E8%AF%95/</link>
    <description>Recent content in 测试 on 大峰哥</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Fri, 05 Jul 2024 02:47:19 +0000</lastBuildDate>
    <atom:link href="https://wfsui.github.io/tags/%E6%B5%8B%E8%AF%95/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>基于UI交互意图理解的异常检测方法</title>
      <link>https://wfsui.github.io/posts/%E5%9F%BA%E4%BA%8Eui%E4%BA%A4%E4%BA%92%E6%84%8F%E5%9B%BE%E7%90%86%E8%A7%A3%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95/</link>
      <pubDate>Fri, 05 Jul 2024 02:47:19 +0000</pubDate>
      <guid>https://wfsui.github.io/posts/%E5%9F%BA%E4%BA%8Eui%E4%BA%A4%E4%BA%92%E6%84%8F%E5%9B%BE%E7%90%86%E8%A7%A3%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95/</guid>
      <description>1. 背景 近年来，随着美团多种业务线的扩充和迭代，UI测试的任务愈发繁重。针对UI测试中人工成本过高的问题，美团到店测试团队开发了视觉自动化工具以进行UI界面的静态回归检查。然而，对于UI交互功能逻辑的检验仍强依赖于脚本测试，其无法满足对于进一步效率、覆盖面提升的强烈需求。主要难点体现在两方面：&#xA;前端技术栈多样，不同页面的实现方式各异，这导致不同页面中功能相似的UI模块的组件树差异很多，基于规则的测试脚本也就很难具备泛化能力，生产、维护的成本非常高。 UI样式繁多，同样的功能模块可能在视觉上有很大差异，这为基于CV方法实现自动化驱动带来了困难。 考虑上述两个难点，美团到店平台技术部/质量工程部与复旦大学计算机科学技术学院周扬帆教授团队展开了“基于UI交互理解的智能化异常检测方法”的科研合作，利用多模态模型对用户可见文本、视觉图像内容和UI组件树中的属性进行融合，实现了对于UI交互意图[1]的准确识别。该工作对于大前端UI的质量保障等多个领域都具有可借鉴的意义，介绍该工作的学术论文[2]已经被 ESEC/FSE 2023 （软件领域CCF A类推荐会议）接收，并将于12月6日在其工业届轨（Industry track）公开发布、推介。&#xA;2. UI交互介绍 2.1 UI模块与交互意图 移动应用由“页面”组成，不同页面中的不同“模块”为用户提供着不同的功能。用户在浏览页面时，根据以往使用经验以及当前页面中的图像、文字、页面结构等信息，可快速理解页面当中不同【模块】所想要提供的【功能】，以及通过该功能用户能够达到的【目的】。这些被用户认为能够提供特定功能并达到预期目的的页面模块，我们将其命名为一个【交互意图簇】。&#xA;以下图中的页面为例，不同模块通常对应不同的交互意图类型划分。比如商品详情区域，我们可以得知此模块主要是向我们展示当前商品最主要的信息，起展示作用；而顾客信息区域，需要用户进行点击或输入个人信息，用以补全整个订单所需要的信息；同时页面当中也会存在各类功能按钮，通过按钮的 位置 、文本信息、图标等信息，用户也可以大致推断出操作后会得到怎样的结果。由此，我们可以将UI交互意图定义为「用户通过当前UI展示推断出来的不同模块的概念及交互功能」。&#xA;2.2 当下痛点与启示 对于复杂的UI交互场景，如提交订单页，测试人员需要对不同模块制定较复杂的测试流程、测试规则，同时编写及维护复杂的自动化测试逻辑。以美团内的App测试场景为例，许多不同的页面有着相似的功能模块，这些功能模块尽管表象不同，但对于一般用户来说，交互意图明确且相似，没有理解困难。如：</description>
    </item>
    <item>
      <title>基于接口数据变异的App健壮性测试实践</title>
      <link>https://wfsui.github.io/posts/%E5%9F%BA%E4%BA%8E%E6%8E%A5%E5%8F%A3%E6%95%B0%E6%8D%AE%E5%8F%98%E5%BC%82%E7%9A%84app%E5%81%A5%E5%A3%AE%E6%80%A7%E6%B5%8B%E8%AF%95%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Fri, 05 Jul 2024 02:47:16 +0000</pubDate>
      <guid>https://wfsui.github.io/posts/%E5%9F%BA%E4%BA%8E%E6%8E%A5%E5%8F%A3%E6%95%B0%E6%8D%AE%E5%8F%98%E5%BC%82%E7%9A%84app%E5%81%A5%E5%A3%AE%E6%80%A7%E6%B5%8B%E8%AF%95%E5%AE%9E%E8%B7%B5/</guid>
      <description>01 什么是客户端健壮性 在维基百科的定义中，健壮性（Robustness）是指一个计算机系统在执行过程中处理错误，以及算法在遭遇输入、运算等异常时继续正常运行的能力。IEEE中将健壮性定义为系统或组件在存在无效输入或压力环境条件下可以正常运行的程度。早在1989年，Barton Miller首次提出了模糊测试的概念，通过向目标应用抛出随机字符串的方式来测试UNIX应用程序的健壮性；而在1996年的Ballista项目中，研究人员探索根据API定义的数据类型，对操作系统或软件接口进行自动化测试方法。两个项目均以“无应用程序崩溃或挂起”作为测试验证通过的标准。&#xA;在移动端App领域，健壮性可以理解为App运行时遭遇环境异常或者输入异常时客户端能够继续正常运行的能力。&#xA;其中，环境异常主要分为操作系统异常、外部环境异常、硬件环境异常三大类。比如内存不足、CPU负载过高、线程池满载、内存分配失败、网络连接失败等。输入异常主要分为系统输入和用户输入。比如网络接口返回的数据异常、应用内缓存、数据库文件读写异常，这类的异常属于在系统输入异常；在电话号码输入框场景，用户输入的空格、富文本则属于用户输入异常。&#xA;对于这些风险，如果App没有处理，理论上都可能会产生展示异常、交互异常、性能、安全等问题，导致用户无法继续使用或在使用过程中产生不好的体验。比如用户操作App下单过程中，API请求出现故障未返回状态码为200的响应，App由于没有获取到预期接口响应的信息而发生崩溃，就会中断用户的使用流程。&#xA;02 基于接口数据变异的App健壮性测试方案设计 在实际的客户端测试执行过程中，测试人员会考虑测试异常输入的场景，但由于成本无法做到无穷尽的测试，同时还存在人工执行遗漏的风险。&#xA;从美团App平台业务的历史故障分析中，我们发现：网络请求返回的数据与实现预期不符引发的Crash或核心功能缺失问题导致的故障占比最高，且影响面较广。比如接口返回非预期数据时，客户端处理数据类型转换异常导致闪退，即使5分钟内操作降级仍影响了百万量级的用户。因此美团平台业务App的健壮性测试探索优先从发现网络请求返回数据导致的异常开始。&#xA;针对于发现请求接口返回客户端非预期数据导致的Crash，或者核心模块缺失问题这个诉求，我们调研后发现方案的基本原理都是相似的，即以网络请求的原始响应为基础，根据规则进行变异构造，使用代理工具改写响应体返回给客户端，在端上设备做异常检测。但是都存在一些问题不能满足诉求，比如测试变异数据是根据预置或者自定义规则随机生成组合，随机性过大，不能有效拦截健壮性问题；但如果不做随机，产生的用例组合量过大，测试不能在合理时间范围内结束；另外在检测能力方面，不具备发现业务异常或功能模块异常的能力。&#xA;因此，我们结合通用方案做了一些自定义改造，整体检测方案包含静态检测和动态检测两部分。&#xA;静态检测，主要是指静态代码扫描，将典型代码编写规范问题转化为自定义静态代码扫描规则，管控增量代码，同时长期治理存量风险。比如自定义了PrimitiveParseDetector、ColorParseDetector，管控业务必须使用健壮性测试通过的工具类。 动态检测是指结合触发时机，构造并注入变异数据后，识别App运行时是否出现崩溃、挂起或业务功能模块异常。比如在集成事件/回归事件触发自动化测试运行，构造触发异常的数据进行动态测试，然后监测是否出现了异常。核心动作包含构造变异数据和完成检测两部分。比如将接口响应体中表示颜色含义的Key对应的Value值构造成非色值，然后检测客户端请求处理接口数据时是否出现崩溃或挂起。 下文重点介绍端到端的动态检测方案。&#xA;03 变异数据的构造和异常检测 对于美团App来说，首页有多种形态，对于某种特定形态，除了控制请求数据外还需要控制实验、策略等一系列因素，才能保证测试对象的唯一性。一个页面中包含多个异步请求，因此请求的构造也需要和页面路径关联。这些都是采集变异所需的基础数据时需要关注和控制的。&#xA;响应体由基本类型数据和复合类型数据组成，相同基本类型的数据可能具备不同的业务语义，需要根据语义的类型做变异规则的区分对待，才能保障业务场景覆盖。</description>
    </item>
    <item>
      <title>小程序可测性能力建设与实践</title>
      <link>https://wfsui.github.io/posts/%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%8F%AF%E6%B5%8B%E6%80%A7%E8%83%BD%E5%8A%9B%E5%BB%BA%E8%AE%BE%E4%B8%8E%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Fri, 05 Jul 2024 02:47:12 +0000</pubDate>
      <guid>https://wfsui.github.io/posts/%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%8F%AF%E6%B5%8B%E6%80%A7%E8%83%BD%E5%8A%9B%E5%BB%BA%E8%AE%BE%E4%B8%8E%E5%AE%9E%E8%B7%B5/</guid>
      <description>1. 引言 测试活动从本质上可以视为被测系统因为某个激励产生相应的响应，并对这些响应进行全面检测的过程。这个过程（激励-&amp;gt;响应-&amp;gt;检查）涉及到两个角色：测试者以及测试对象，测试者执行激励与检查响应，由机器（程序）或者人来完成；被测对象接受激励，产生响应。从这个过程来看：激励可控，响应可观，称之为可测。以实际业务测试为例，修改缓存、网络请求MCOK、页面跳转、用户登录态设置等都属于可测性能力。&#xA;在未经过任何可测性改进的终端产品中，测试人员只能通过UI交互，从UI界面观察来完成最基本的质量保障。然而应用内部存在各种各样复杂的逻辑、状态，要进行更加深入的测试则需要对这些信息进行介入与观测。例如，在进行打点测试时，操作页面后，需确认打点信息是否被正常上报，这一过程通常依赖网络代理调试工具来完成校验。同样，在用户登录测试环节中，登录完成后，需要检查缓存是否已正确记录登录信息，这要求具备缓存查看的能力，这些体现了实际业务测试场景对可测性能力的需求。&#xA;整体而言，完备地构造出目标场景进行测试涉及到多个复杂的方面，同时观测它是否符合预期也比较困难，如下图所示。终端测试长期面临着挑战。为应对这些挑战，我们以增强可测性为基础，将其贯穿测试活动的始终，使得测试能更细粒度地检查系统，提高测试深度和效率。&#xA;作为终端产品的一种形态，小程序是运行在宿主应用（如微信、快手、百度等）之上的“轻应用”，在2017年由微信推出后发展迅速。由于小程序非常依赖于宿主应用环境，因此在测试过程中，除了面临终端测试固有的难点外，它还存在一些特殊的影响因素。&#xA;从运行机制的角度来看，小程序的代码逻辑运行在宿主应用提供的容器环境内，它无法直接控制宿主应用本身和手机系统，这在一定程度上增大了测试与可测性改进的难度。&#xA;在目前的实践中，针对小程序的测试主要存在以下几种工具和策略：&#xA;采用如Charles、Fiddler等网络代理工具进行HTTP/HTTPS请求和响应的代理分析与校验。虽然这类工具适合进行数据包的抓取和分析，但它们通常无法深入小程序的内部架构，因此无法全方位控制或感知应用的内部状态。 运用图像处理技术的自动化测试工具如Airtest进行测试，它们主要关注于界面层面的操作，未能触及应用程序背后的逻辑处理，因此仍属于“黑盒测试”的范畴。 利用微信官方提供的Minium小程序测试工具来执行更为精细的测试操作，能够进行诸如API Mocking等内部控制。然而，该方法操作复杂，并依赖于微信开发者工具，而后者与真机环境之间存在一定差异，可能影响测试结果的准确性。 开发专用的自研调试面板用以验证程序逻辑和测试特定场景，但这些工具设计时常常专注于特定小程序，不易迁移至其他应用，而且它们通常不支持自动化测试流程。 综上所述，尽管存在多种测试工具和方法，但目前尚缺乏一套综合性的、易于使用的测试工具集，能够全面提升小程序的可测性。&#xA;2. 小程序可测性介绍 小程序可测性的目标在于构建一套全方位的通用小程序可测性能力集合。该体系无缝支持真机和模拟器环境，兼容多端、多平台，并允许不同应用以低成本轻松接入。它能深入核心，为小程序提供全面而多元的可观测性与可控性，覆盖应用界面、内部状态、存储等关键领域。这一体系旨在赋能测试者更便捷地应对复杂测试场景，显著提高测试的效率与深入度。&#xA;经过了长期的建设积累，目前我们已经构建了一套比较全面的终端可测性能力集，包含Android、iOS、小程序、Web等技术栈。其中小程序由于系统的结构特殊性，可测性能力相对其它端会有一些不同。小程序可测性主要包括业务逻辑可测性、应用可测性、系统&amp;amp;设备可测性三个层级，在每个层级中包含多个垂直的细分方向，除了支持多技术栈的公共可测性能力，还提供了如AppData、宿主应用信息可观可控等特有能力。下面以几个典型能力说明小程序可测性使用方式与效果。&#xA;2.1 使用方式与效果 在实际的手工以及自动化测试工作中，小程序可测性能力能够很方便的使用，并在多个场景下发挥了重要的作用。</description>
    </item>
    <item>
      <title>基于模式挖掘的可靠性治理探索</title>
      <link>https://wfsui.github.io/posts/%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%BC%8F%E6%8C%96%E6%8E%98%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%B2%BB%E7%90%86%E6%8E%A2%E7%B4%A2/</link>
      <pubDate>Fri, 28 Jun 2024 02:48:55 +0000</pubDate>
      <guid>https://wfsui.github.io/posts/%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%BC%8F%E6%8C%96%E6%8E%98%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%B2%BB%E7%90%86%E6%8E%A2%E7%B4%A2/</guid>
      <description>1 可靠性治理的痛点 对于亿级流量的线上系统来说，可靠性是至关重要的。从字面上理解，可靠性要求故障少、可信赖。与安全性一样，它们都是信息系统的固有属性之一，也是保障产品质量的关键因素。&#xA;对照Google的可靠性模型来看，测试同学会投入很多精力在用例设计、测试执行、持续交付等环节上，研发同学则会更多关注监控、应急和故障分析等。但往往由于项目进度和人力因素，在设计和编码阶段对可靠性的投入和关注不足，导致后续需要付出更高的成本发现和解决潜在隐患。有鉴于此，我们希望能找到更低成本且以更有效的方式发现和治理这些隐患，从而提升系统整体的可靠性。&#xA;在研发设计阶段，我们需要关注系统弹性，考虑潜在故障风险、适应流量变化等，其中相关治理涉及幂等性、健壮性、一致性、超时、限流、熔断等场景。与一般功能测试相比，可靠性治理需要面对不同的服务和系统，发现并治理技术问题，在模糊度上有较大的提升和挑战。就目前而言，质量问题非常明确，但潜在风险策略和解决路径比较模糊。因此，我们希望能找到办法识别并解决这些问题。&#xA;模糊度的提升会带来两种最常见的现象：&#xA;一种是过于具体，Case by Case解决问题，类似算法的过拟合，过拟合的问题在于对更广泛范围内的问题缺乏有效性。以幂等性为例，想验证一个接口是否幂等可以很快完成并很快补充接口幂等相关的测试用例，但是对不同的接口、服务、系统以及不同的幂等性设计，还有哪些问题和风险，我们没有办法关注到并控制这些风险。 另一种是过于泛化，类似算法的欠拟合，欠拟合的问题在于过度虚化导致没有抓住问题的共性特征。以主从延迟为例，主从延迟会给系统带来一致性风险，需要针对性做保护，并进行相关验证，因此我们可以制定规范、梳理Check List和测试模板，虽然这样可以最大程度在产研各环节提醒大家关注到这类问题，但并没有找到彻底解决问题的方法。 2 模式的定义 类似这些问题如何找到更好的解决办法？我们重点看一下模式对可靠性治理的启发。模式在维基百科的定义是：揭示了这个世界上人工设计和抽象思想中的规律。&#xA;例如下图所示，计算机图形学中的经典分形图案柯赫雪花，是1904年瑞典数学家科赫提出。可以看到它有明显的规律，这样的分形规律在自然界无处不在。&#xA;技术场景的模式会更加丰富些，这类模式和可靠性治理想找到的模式非常接近。&#xA;举例缓存设计的两种常见模式：&#xA;第一种是Cache-Aside（旁路缓存），也是使用比较广泛的一种方式，它只有在缓存没有命中时，才会查询数据库并更新缓存。 另外一种是Write-throught（只写模式），这种模式在每次数据库变更时都会同步更新缓存。 对比第一种模式，第二种模式的优点是逻辑更清晰、操作简单、缓存命中率更高；缺点是不常请求的数据会被写到缓存中，导致缓存更大。</description>
    </item>
    <item>
      <title>代码变更风险可视化系统建设与实践</title>
      <link>https://wfsui.github.io/posts/%E4%BB%A3%E7%A0%81%E5%8F%98%E6%9B%B4%E9%A3%8E%E9%99%A9%E5%8F%AF%E8%A7%86%E5%8C%96%E7%B3%BB%E7%BB%9F%E5%BB%BA%E8%AE%BE%E4%B8%8E%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Sun, 23 Jun 2024 02:48:18 +0000</pubDate>
      <guid>https://wfsui.github.io/posts/%E4%BB%A3%E7%A0%81%E5%8F%98%E6%9B%B4%E9%A3%8E%E9%99%A9%E5%8F%AF%E8%A7%86%E5%8C%96%E7%B3%BB%E7%BB%9F%E5%BB%BA%E8%AE%BE%E4%B8%8E%E5%AE%9E%E8%B7%B5/</guid>
      <description>1 软件系统风险与变更 变更是软件系统进化的推动力，同时也是孕育风险的温床。如果一个系统没有了相应的迭代和变更，那这个系统就会逐渐失去了活性和价值。不过，随着系统进行了变更迭代，软件风险也会慢慢衍生，而规避变更引发的软件风险在质量保障领域是一个较大的挑战。通过对下面典型软件系统架构图分析，我们可提炼出3大类变更维度：&#xA;基础设施变更：主要包括基础硬件变更、运营商网络变更、云服务容器变更、开发语言变更、操作系统变更以及机房集群的变更，这些基础设施迭代极大提升了系统底层的服务能力，一旦变更引发系统风险，其影响面通常也比较大。 系统外部变更：比如用户流量突增、用户需求变化以及相关三方服务及三方组件变更，这些帮助系统不断衍生出新的迭代能力，同时也增加了系统稳定性风险的发生。 系统内部变更：比如技术人员迭代、新功能发布以及系统整体架构的升级等，这是驱动系统软件进化的核心变更因子，也是最频繁的变更风险发生地。 在这里，我们先列举了一些比较常见的、因变更风险所引发的典型线上事故：&#xA;外部变更所引发的线上问题，某地的光缆被挖断导致整个服务有很大的影响。 代码变更典型问题，谷歌Gmail系统在发布新功能时产生的副作用而引发的功能上问题。 代码变更典型问题，Knight公司在升级一段很老的代码时引发的异常逻辑功能发生。 配置变更引发的问题，所引发的“薅羊毛”事件。 人员操作变更，研发误操作引发的整个核心数据删除； 可以看到，在实际的工作中，由变更所引发的风险，对业务的冲击非常大。结合美团亿级流量的到家业务形态看，系统变更引发风险可能性进一步放大，变更风险的“蝴蝶效应”更加凸显，某个单点问题都有可能给整个到家核心业务带来极大的影响。&#xA;第一，从到家业务接入方看，美团内部业务包括外卖、闪购、医药等等，外部有众多的企业客户。 第二，系统参与相关方较多，包括C端用户、商家、配送骑手及各个平台。 第三，业务基于微服务架构模式，各个业务间调用关系复杂，核心链路非常长。另外，业务强依赖配置，一旦某个环节发生变更问题，相关方都会受到影响。 所以对研发与测试来说，洞察与规避变更引入的质量风险变得至关重要。&#xA;那么，关于变更风险，质量建设核心做功点在哪里？我们对历史线上问题分析发现，系统内部变更引发故障的占比较高，且变更与代码变更有直接或间接关系。因此，我们开始围绕代码变更这个核心变更因子，构建了质量建设的做功点。&#xA;随后，我们思考了两个关键问题：&#xA;代码变更风险是否可被可视化，提升测试和研发感知能力。 围绕代码变更风险，是否能够构建一套质量保障防御体系。 通过分析发现，结合下图的代码特征树，我们就可以更好地感知代码变更的可视化能力。然后通过各叶子节点，将所有相关特征很好地识别，并且做对应的质量防御策略。</description>
    </item>
    <item>
      <title>自动化测试在美团外卖的实践与落地</title>
      <link>https://wfsui.github.io/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95%E5%9C%A8%E7%BE%8E%E5%9B%A2%E5%A4%96%E5%8D%96%E7%9A%84%E5%AE%9E%E8%B7%B5%E4%B8%8E%E8%90%BD%E5%9C%B0/</link>
      <pubDate>Thu, 09 Feb 2023 03:01:33 +0000</pubDate>
      <guid>https://wfsui.github.io/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95%E5%9C%A8%E7%BE%8E%E5%9B%A2%E5%A4%96%E5%8D%96%E7%9A%84%E5%AE%9E%E8%B7%B5%E4%B8%8E%E8%90%BD%E5%9C%B0/</guid>
      <description>1. 项目背景 美团外卖的业务场景比较多元化，除了外卖自身的业务，还作为平台承接了闪购、团好货、医药、跑腿等其他业务。除此之外，在全链路动态化的大基调下，外卖各个页面的技术形态也变得越来越复杂，除了Native代码，还包括Mach（外卖自研动态化框架）、React Native、美团小程序、H5等，不同技术栈的底层技术实现不同，渲染机制不同，进而对测试方式要求也有所不同，这也在无形中增加了测试的难度。下图汇总了美团多业务、多技术、多App的一些典型场景。&#xA;在产品交付上线的过程中，测试的占比也是非常大的，甚至大于总时长的30%。如下图所示，整个测试包括了冒烟测试、新功能测试、二轮回归测试、三轮测试。然而，现在需求测试绝大部分还是采用非自动化的方式，这就使得人力成本变得非常之高。&#xA;另一方面，相比于2018年，2022年的测试用例数量增长近3倍，已经超过1万2千条（如下图所示）。同时，外卖的业务是“三端复用”，除了外卖App，还需要集成到美团App和大众点评App上，这样一来，测试工作量就翻了3倍，业务测试压力之大可想而知。如果按照当前的增长趋势持续下去，要保障外卖业务的稳定，就必须持续不断地投入大量的人力成本，所以引入能够支持外卖“多业务场景”、“多App复用”、“多技术栈” 特点的自动化测试工具来提升人效和质量，势在必行。&#xA;2. 项目目标 为了解决外卖面临的测试困境，我们尝试去探索一种零学习成本、低维护、高可用的自动化测试方案，能够支持外卖复杂多变的测试场景，它必须同时满足下面几点要求：&#xA;易用性：工具/平台的上手难度，使用复杂度应该尽可能的低，因为自动化测试的目的是提效人力，而不是增加人力负担。 平台支持：移动端至少需要覆盖iOS和Android双平台，同时基于外卖的业务特点，不仅需要对Native支持，也需要支持Mach（自研局部动态化框架）、H5、React Native、美团小程序等技术栈。 稳定性：自动化测试用例的执行需要有足够的稳定性和准确性，测试过程中不应因测试工具本身的不稳定而出现稳定性问题。 维护成本：维护成本很大程度上决定了测试工作量的大小，因需求产生变动或架构重构等问题时，用例的维护成本应该尽可能的小。 可扩展性：当测试方案不能满足测试需求时，工具/平台应具备可扩展的能力。 3. 方案选型 自动化测试工具那么多，自研是重复造轮子吗？&#xA;针对终端的UI自动化测试工具/平台可谓“屡见不鲜”，市面上也有很多相对成熟的方案，相信大家都有用过，或者至少有所耳闻，但这些方案是否能真的满足我们提效的诉求呢？以下我们挑选了三类非常具有代表性的自动化测试工具/平台 - Appium、Airtest Project、SoloPi进行了分析，来帮助大家对自动化测试技术建立一个认知：</description>
    </item>
  </channel>
</rss>
